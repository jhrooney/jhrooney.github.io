[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jacob Rooney",
    "section": "",
    "text": "I’m a mathematician interested in machine learning.\n\nEducation\nUniversity of California, Los Angeles | Los Angeles, CA\nPh.D. in Mathematics | 2012—2018\nUniversity of California, Los Angeles | Los Angeles, CA\nM.S. in Mathematics | 2014\nUniversity of Georgia | Athens, GA\nB.S. in Mathematics | 2008—2012\n\n\nExperience\nSimons Center for Geometry and Physics | Stony Brook, NY\nResearch Assistant Professor (Postdoc) | 2020—2023\nUniversity of California, Los Angeles | Los Angeles, CA\nAssistant Adjunct Professor (Postdoc) | 2019—2020\nUniversity of Southern California | Los Angeles, CA\nAssistant Professor (RTPC) (Postdoc) | 2018—2019"
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "",
    "text": "The goal of this post is to do some data cleaning on the Oxford-IIIT Pets dataset, an image classification dataset with photos of cats and dogs. We’ll get our results using image embeddings from the body of a pre-trained ResNet model, some basic machine learning and image processing techniques, and a full ResNet model fine-tuned on the dataset using fastai. We’ll use fastai’s version of the Pets dataset and take advantage of some convenience functions from the fastai and fastcore libraries.\n\nimport os\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport cv2\n\nimport torch\nimport torchvision\nimport torch.nn.functional as F\n\nimport fastcore.all as fc\n\nThe list of Python packages and specific versions used to make this post are in the table below.\n\n\n\n\n\nPackage\nVersion\n\n\n\n\npython\n3.9.16\n\n\nscipy\n1.9.2\n\n\nfastai\n2.7.13\n\n\npytorch\n2.1.2+cu121\n\n\nfastcore\n1.5.29\n\n\ntorchvision\n0.16.2+cu121\n\n\nopencv-python\n4.9.0"
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#image-similarity-search",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#image-similarity-search",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Image Similarity Search",
    "text": "Image Similarity Search\nBefore we go any further, let’s implement a basic image similarity search function over the dataset. If our embeddings can’t accomplish that task, then using them to look for duplicates will be a waste of time. We’ll measure how similar two images are using the cosine similarity distance between their embeddings.\n\ndef pairwise_cosine_similarities(embs, batch_size=8, device=device):\n    embs = embs.to(device)\n    num_embs = embs.shape[0]\n    distances = torch.empty((num_embs, num_embs), device=device)\n    for idx in range(0, num_embs, batch_size):\n        s = slice(idx, min(idx+batch_size, num_embs))\n        distances[s] = F.cosine_similarity(embs[None], embs[s, None], dim=-1)\n    # don't match dataset entries with themselves\n    distances.fill_diagonal_(-torch.inf)\n    return distances.to('cpu')\n\ndistances = pairwise_cosine_similarities(embeddings)\n\nNow let’s look at some examples and see if our embeddings are worth anything.\n\ndef show_k_closest(idx, dataset, distances, k=4):\n    item_metrics = distances[idx]\n    k_closest_idxs = item_metrics.argsort(descending=True)[:k]\n    k_closest_imgs = dataset[k_closest_idxs]\n    relevant_imgs = [dataset[idx]] + k_closest_imgs\n    show_image_list(\n        relevant_imgs,\n        max_per_col=k+1,\n        title_option='label_original',\n    )\n\nshow_k_closest(500, pets_dataset, distances)\nshow_k_closest(3303, pets_dataset, distances)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL supports numpy-style indexing\nLooks good!"
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#duplicate-and-near-duplicate-images",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#duplicate-and-near-duplicate-images",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Duplicate and Near-Duplicate Images",
    "text": "Duplicate and Near-Duplicate Images\nNow let’s go ahead and find groups of similar images to hunt for duplicates. We’ll threshold the distances we computed earlier to get a boolean matrix, which we can view as the adjacency matrix of a graph whose vertices are the entries of the dataset.2 The connected components of that graph with more than one vertex are our desired groups.\n\nfrom scipy.sparse.csgraph import connected_components\n\ndef group_similar_images(distances, threshold, dataset):\n    graph = (distances &gt; threshold).numpy()\n    num_components, labels = connected_components(graph)\n    components = fc.L(dataset[labels == i] for i in range(num_components))\n    groups = components.filter(lambda o: len(o) &gt; 1)\n    return groups\n\ngroups = group_similar_images(distances, 0.85, pets_dataset)\n\n\n\nL has a built-in filter method.\nHow many images did we find?\n\nprint(f'number of groups: {len(groups):3d}\\n'\n      f'number of images: {len(groups.concat())}')\n\nnumber of groups:  92\nnumber of images: 192\n\n\n\n\nL has a built-in concat method that concatenates all of its elements into a new L\nWe can check and see that the groups we found really are duplicates or near-duplicates. We’ll only show a selection of the groups we found; the remaining ones are similar.\n\nshow_image_list(groups[20, 52, 44, 16, 2].concat(), max_per_col=6)\n\n\n\n\n\n\n\n\nLet’s update our list of images to remove all but one member from each group.\n\nduplicate_images = set(fc.flatten(o[1:] for o in groups))\nimages_to_remove.update(duplicate_images)\n\n\n\nas its name suggests, fc.flatten flattens its input, returning a generator"
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#bright-and-dark-images",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#bright-and-dark-images",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Bright and Dark Images",
    "text": "Bright and Dark Images\nOne way to measure the brightness of an image is to convert it to HLS (hue, lightness, and saturation) format. OpenCV has a cvtColor function that does this transformation. They compute lightness in the following way. For each pixel in an RGB image, with values \\((r, g, b)\\), we set \\(V_\\text{max} = \\max(r, g, b)\\) and \\(V_\\text{min} = \\min(r, g, b)\\); then the lightness of that pixel is the average \\((V_\\text{max} + V_\\text{min}) / 2\\).3\n\ndef brightness(img):\n    img = np.asarray(load_image(img, mode='RGB'))\n    img_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    return img_hls[..., 1].mean()\n\nbright_vals = fc.parallel(brightness, pets_dataset, n_workers=4)\n\n\n\nfc.parallel is a convenience multiprocessing function that maps a function passed as the first input over the second input in parallel\nWe’ll filter for bright images using a lower bound on the brightness and only show a few of the brightest images.4\n\nbright_images = pets_dataset[bright_vals.map(fc.gt(231))]\nshow_image_list(bright_images, row_height=4)\n\n\n\n\n\n\n\n\n\n\nfc.gt is a fastcore function that, when passed one parameter, returns a curried version of operator.gt\nThese images are also all clearly identifiable, so we’ll keep them too.\nWe can also filter for dark images using an upper bound on the brightness. Again, we’ll only show a few of the darkest images.\n\ndark_images = pets_dataset[bright_vals.map(fc.lt(25))]\nshow_image_list(dark_images, row_height=4)\n\n\n\n\n\n\n\n\n\n\nfc.lt is the “less than” analogue of the function fc.gt above\nThese images are all clearly identifiable, so we won’t remove any of them."
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#outliers",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#outliers",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Outliers",
    "text": "Outliers\nWe’ll look for potential outliers by first projecting our embeddings to a 50-dimensional space with principal component analysis and then using a local outlier factor model.5 The value for contamination is set to a low value here to show only a few images.6\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.pipeline import make_pipeline\n\npca = PCA(n_components=50, svd_solver='full')\nclf = LocalOutlierFactor(\n    n_neighbors=4, metric='cosine', n_jobs=4, contamination=0.0005\n)\noutlier_preds = make_pipeline(pca, clf).fit_predict(embeddings.numpy())\noutlier_candidates = pets_dataset[outlier_preds == -1]\nshow_image_list(outlier_candidates, row_height=4)\n\n\n\n\n\n\n\n\nOne of these images looks like a problem: great_pyrenees_36 contains both a cat and a dog, so we should remove it from the dataset.\n\nimages_to_remove.add(outlier_candidates[0])"
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#blurry-images",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#blurry-images",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Blurry Images",
    "text": "Blurry Images\nWe’ll measure blurriness using the squared Euclidean norm of the image Laplacian. A sharp image tends to have many distinct edges and boundaries in it, and the image Laplacian detects those features.\n\ndef compute_laplacian(img):\n    img = np.asarray(load_image(img, mode='L'))\n    return cv2.Laplacian(img, cv2.CV_64F, ksize=5)\n\nThe way the Laplacian detects blurry images is most clearly seen through examples. We’ll start with a relatively sharp image…\n\nsharp_img = pets_dataset[59]\nsharp_laplacian = compute_laplacian(sharp_img)\nshow_image_list([sharp_img, sharp_laplacian], col_width=11, row_height=6)\n\n\n\n\n\n\n\n\n… and contrast that with a relatively blurry image.\n\nblurry_img = pets_dataset[5674]\nblurry_laplacian = compute_laplacian(blurry_img)\nshow_image_list([blurry_img, blurry_laplacian], col_width=6, row_height=4)\n\n\n\n\n\n\n\n\nSharp images tend to have a Laplacian with a greater squared norm than blurry images. We’ll just compute it for each image and take those with the lowest values as our candidate blurry images. I’ll only show a few of the images whose Laplaicans have the smallest squared norms.7\n\ndef laplacian_norm_squared(img):\n    laplacian = compute_laplacian(img)\n    return (laplacian**2).sum()\n\nsquared_norms = fc.parallel(laplacian_norm_squared, pets_dataset, n_workers=4)\nthreshold = np.quantile(squared_norms, q=0.0005)\nblurry_candidates = pets_dataset[squared_norms.map(fc.lt(threshold))]\nshow_image_list(blurry_candidates, max_per_col=4, row_height=4)\n\n\n\n\n\n\n\n\n\n\nL has a built-in map method\nEach of these images is identifiable, so we’ll keep them all."
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#fine-tune-a-model",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#fine-tune-a-model",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Fine-Tune a Model",
    "text": "Fine-Tune a Model\nFinally, we’ll use fastai to quickly fine-tune a breed classification model on the Pets dataset and examine the images with the greatest losses to check for any other issues. We’ll use fastai’s DataBlock API to organize our data into a DataLoaders and the vision_learner convenience function to create a trainer that we’ll use to fine-tune a pre-trained ResNet50 model.8 One of fastai’s best features is its learning rate finder, which, as the name suggests, helps us find an advantageous learning rate.\n\nimport fastai.vision.all as fv\n\ndef get_breed(filename):\n    return '_'.join(filename.stem.split('_')[:-1])\n\ndblock = fv.DataBlock(\n    blocks=(fv.ImageBlock, fv.CategoryBlock),\n    get_items=fc.noop,\n    get_y=get_breed,\n    item_tfms=fv.Resize(460),\n    batch_tfms=[*fv.aug_transforms(size=224, min_scale=0.75),\n                fv.Normalize.from_stats(*fv.imagenet_stats)],\n)\ndls = dblock.dataloaders(pets_dataset)\nlearn = fv.vision_learner(dls, fv.resnet50, metrics=fv.accuracy)\nlrs = learn.lr_find(suggest_funcs=(fv.minimum, fv.steep, fv.valley, fv.slide))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt looks like the loss is decreasing most quickly at the point marked steep, so we’ll use that learning rate. We’ll use the Learner’s fine_tune method to first freeze the weights in the model’s body and train for one epoch, then unfreeze everything and train for five epochs.\n\nlearn.fine_tune(5, lrs.steep)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.126889\n0.367208\n0.889716\n00:17\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.419063\n0.284448\n0.921516\n00:20\n\n\n1\n0.365594\n0.341063\n0.899188\n00:20\n\n\n2\n0.255289\n0.224743\n0.929635\n00:20\n\n\n3\n0.140554\n0.210314\n0.932341\n00:20\n\n\n4\n0.081783\n0.204320\n0.939784\n00:20\n\n\n\n\n\nWe can use fastai’s Interpretation class to look at the images where the outputs from the model have the greatest losses.\n\nplt.rcParams.update({'font.size': 8})\ninterp = fv.Interpretation.from_learner(learn)\ninterp.plot_top_losses(k=9, nrows=3, figsize=(15, 9))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere don’t seem to be any issues with the images themselves, so we won’t remove anything from the dataset."
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#footnotes",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#footnotes",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI tried out a few reasonably sized pre-trained models from torchvision, fastai, and timm, and the embeddings from this Torchvision ResNet50 model seemed to perform the best on image similarity search and finding duplicate images.↩︎\nI chose the threshold of 0.85 to try to catch as many duplicate groups as I could while making sure that near-duplicate groups had only small changes between the images.↩︎\nI found that OpenCV’s implementation was faster than using the formula directly.↩︎\nI didn’t find any problem images with a lower threshold either.↩︎\nOf the outlier detection methods I tried, this was the one that most easily found the problem image below.↩︎\nI didn’t find any additional problem images with higher values for contamination.↩︎\nI didn’t find any problem images with higher threshold values.↩︎\nWe’re using fastai’s ResNet50 pre-trained weights now since they fine-tuned more easily and to a higher accuracy than the Torchvision weights we used earlier.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacob Rooney",
    "section": "",
    "text": "I’m a mathematician interested in machine learning.\n \n  \n   \n  \n    \n     GitHub"
  },
  {
    "objectID": "index.html#a-little-about-me",
    "href": "index.html#a-little-about-me",
    "title": "Jacob Rooney",
    "section": "A Little About Me",
    "text": "A Little About Me\nAfter getting a PhD in mathematics at UCLA, I spent several years in academia before becoming interested in machine learning. In particular, I want to see what practical insights and advancements mathematics can bring to the machine learning world. You can read more detailed information about me here."
  },
  {
    "objectID": "index.html#recent-blog-posts",
    "href": "index.html#recent-blog-posts",
    "title": "Jacob Rooney",
    "section": "Recent Blog Posts",
    "text": "Recent Blog Posts\nI recently started to blog about the projects I’m working on. My most recent ones are listed below, and you can check out all of them here.\n\n\n\n\n\n\n\n\n\n\nUsing Hugging Face Image Datasets with fastai\n\n\n\n\n\nWe’ll look at different ways of using fastai’s data pipeline with Hugging Face image datasets and find an efficient option.\n\n\n\n\n\nFeb 12, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning an Image Dataset with ResNet Embeddings and fastai\n\n\n\n\n\nWe’ll clean up some problems in the Oxford-IIIT Pets dataset.\n\n\n\n\n\nJan 8, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2024-02-12-hf-datasets-with-fastai/index.html",
    "href": "blog/posts/2024-02-12-hf-datasets-with-fastai/index.html",
    "title": "Using Hugging Face Image Datasets with fastai",
    "section": "",
    "text": "Overview and Setup\nThe goal of this post is to find an efficient way of using fastai’s data pipeline tools when working with Hugging Face datasets. As it turns out, the DataBlock and mid-level APIs have some drawbacks in this context, and we’ll see that a better option is to use the low-level API: fastai’s custom DataLoader and Pipeline classes. If you just want to see how to use the low-level API, feel free to skip to that section.\n\nimport torch\nfrom datasets import load_dataset, load_dataset_builder\n\nThe list of Python packages and specific versions used to make this post are in the table below.\n\n\n\n\n\nPackage\nVersion\n\n\n\n\npython\n3.9.16\n\n\nPyTorch\n2.2.0+cu121\n\n\nfastai\n2.7.14\n\n\ntimm\n0.9.12\n\n\ndatasets\n2.15.0\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe latest version of the datasets library at time of writing is 2.16.1, but some of the dataset metadata was missing when using both versions 2.16.0 and 2.16.1.\n\n\n\n\nGet the Metadata\nWe’ll be using the Fashion-MNIST dataset in our analysis. It’s a drop-in replacement for the original MNIST dataset. We’ll use the version of Fashion-MNIST from the Hugging Face Hub. One nice feature of Hugging Face’s datasets library is the DatasetBuilder class that contains metadata for a given dataset. You can use load_dataset_builder to download a DatasetBuilder for any dataset on the Hugging Face Hub before downloading the dataset itself.\n\nfashion_mnist_builder = load_dataset_builder(\"fashion_mnist\")\n\nThe metadata we want is stored in the builder’s info attribute.\n\nfashion_mnist_info = fashion_mnist_builder.info\n\nFor example, there’s a written description of the dataset…\n\nprint(fashion_mnist_info.description)\n\nFashion-MNIST is a dataset of Zalando's article images—consisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.\n\n\n\n… a dictionary of pre-defined splits in the dataset, with relevant metadata…\n\nfashion_mnist_info.splits\n\n{'train': SplitInfo(name='train', num_bytes=31296607, num_examples=60000, shard_lengths=None, dataset_name='fashion_mnist'),\n 'test': SplitInfo(name='test', num_bytes=5233810, num_examples=10000, shard_lengths=None, dataset_name='fashion_mnist')}\n\n\n… and a dictionary of all of the columns in the dataset, with their datatypes and relevant metadata.\n\nfashion_mnist_info.features\n\n{'image': Image(decode=True, id=None),\n 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\nThe ClassLabel object in info.features has a convenient int2str method to convert integer labels to the strings that they represent.\n\nfashion_mnist_label_func = fashion_mnist_info.features[\"label\"].int2str\nnum_classes = len(fashion_mnist_info.features[\"label\"].names)\n{i: fashion_mnist_label_func(i) for i in range(num_classes)}\n\n{0: 'T - shirt / top',\n 1: 'Trouser',\n 2: 'Pullover',\n 3: 'Dress',\n 4: 'Coat',\n 5: 'Sandal',\n 6: 'Shirt',\n 7: 'Sneaker',\n 8: 'Bag',\n 9: 'Ankle boot'}\n\n\n\n\nGet the Data\nWe can also download and load the dataset using our DatasetBuilder.\n\nfashion_mnist_builder.download_and_prepare()\nfashion_mnist = fashion_mnist_builder.as_dataset()\nfashion_mnist\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\nThe splits, features, and number of training and test instances all match the metadata that we looked at in the previous section. We’ll discard the provided test set and divide the training set into a new training and validation set using the Hugging Face Dataset class’s built-in train_test_split method.\n\nfashion_mnist = fashion_mnist[\"train\"].train_test_split(test_size=0.2)\nfashion_mnist\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 48000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 12000\n    })\n})\n\n\n\n\nThe DataBlock API\nOne commonly used data processing API in the fastai library is the DataBlock API. We’ll need to do some additional pre-processing steps when working with a Hugging Face Dataset. The procedure we use is taken from an example in the documentation for blurr, a library that makes it easy to use Hugging Face transformers with fastai. The specific example I’m referencing is the second example in the linked “Prepare the datasets” section.\nStep 1: Record the indices of the validation set.\n\nnum_train, num_valid = len(fashion_mnist[\"train\"]), len(fashion_mnist[\"test\"])\nvalid_idxs = list(range(num_train, num_train + num_valid))\n\nStep 2: Concatenate the training and validation sets into one dataset.\nWe’ll use the concatenate_datasets function from the datasets library.\n\nfrom datasets import concatenate_datasets\n\nconcat_fashion_mnist = concatenate_datasets([fashion_mnist[\"train\"],\n                                             fashion_mnist[\"test\"]])\n\nStep 3: Pass an IndexSplitter when creating our DataBlock so that it knows which instances go in the training and validation sets.\n\nfrom fastai.vision.all import (\n    DataBlock, ImageBlock, CategoryBlock, IndexSplitter, CropPad,\n    RandomCrop, Normalize, imagenet_stats, Rotate, RandomErasing\n)\n\ndef get_x(item):\n    return item[\"image\"]\n\ndef get_y(item):\n    return fashion_mnist_label_func(item[\"label\"])\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_x=get_x,\n    get_y=get_y,\n    splitter=IndexSplitter(valid_idx=valid_idxs),\n    item_tfms=[CropPad(32),\n               RandomCrop(28)],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                Rotate(),\n                RandomErasing()],\n)\n\nNow we can create our DataLoaders.\n\nfashion_mnist_dls = dblock.dataloaders(concat_fashion_mnist, bs=512)\n\nAs usual, we can use fastai’s conveniences like show_batch.\n\nfashion_mnist_dls.show_batch(figsize=(5, 5))\n\n\n\n\n\n\n\n\nThen we can create a Learner and fine-tune a model on our dataset like normal.\n\nfrom fastai.vision.all import (\n    vision_learner, resnet18, accuracy,\n    minimum, steep, valley, slide\n)\n\nfashion_mnist_learn = vision_learner(fashion_mnist_dls, resnet18, metrics=accuracy)\nlrs = fashion_mnist_learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph seems to be steepest at the point marked steep, so we’ll take that as our learning rate.\n\nfashion_mnist_learn.fine_tune(3, lrs.steep)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.324347\n0.824672\n0.700250\n00:15\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.608801\n0.476613\n0.848417\n00:12\n\n\n1\n0.468581\n0.308588\n0.882750\n00:11\n\n\n2\n0.400869\n0.285220\n0.893000\n00:11\n\n\n\n\n\nGreat!\n\n\nThe Downside of the DataBlock API with a Hugging Face Dataset\nEverything looks fine on the surface, but there’s a hidden cost of using the DataBlock API with a Hugging Face Dataset: the time it takes to create the DataLoaders. Let’s re-run our DataBlock pipeline and time it.\n\nimport time\n\nstart_time = time.monotonic()\n\n# Step 1\nnum_train, num_valid = len(fashion_mnist[\"train\"]), len(fashion_mnist[\"test\"])\nvalid_idxs = list(range(num_train, num_train + num_valid))\n\n# Step 2\nconcat_fashion_mnist = concatenate_datasets([fashion_mnist[\"train\"],\n                                             fashion_mnist[\"test\"]])\n\n# Step 3\nfashion_mnist_dblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_x=get_x,\n    get_y=get_y,\n    splitter=IndexSplitter(valid_idx=valid_idxs),\n    item_tfms=[CropPad(32),\n               RandomCrop(28)],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                Rotate(),\n                RandomErasing()],\n)\n\n# instantiate the dataloaders\nfashion_mnist_dls = fashion_mnist_dblock.dataloaders(concat_fashion_mnist, bs=512)\n\nend_time = time.monotonic()\nprint(\"Time elapsed:\", round(end_time - start_time, 1), \"seconds\")\n\nTime elapsed: 18.9 seconds\n\n\nThat’s too long for such a small dataset, and if we create a DataBlock with a bigger dataset like Imagenette, the time required to instantiate the DataLoaders will be even longer. Let’s first download the dataset…\n\nimagenette = load_dataset(\"johnowhitaker/imagenette2-320\")\n\n\n\nMany thanks to Jonathan Whitaker for uploading this version of Imagenette to the Hugging Face Hub!\n… then split the provided training set into a training and validation set…\n\nimagenette = imagenette[\"train\"].train_test_split(test_size=0.2)\nimagenette\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10715\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 2679\n    })\n})\n\n\n… then set up some new functions to use in our DataBlock.\n\nimagenette_label_dict = dict(\n    n01440764='tench',\n    n02102040='English springer',\n    n02979186='cassette player',\n    n03000684='chain saw',\n    n03028079='church',\n    n03394916='French horn',\n    n03417042='garbage truck',\n    n03425413='gas pump',\n    n03445777='golf ball',\n    n03888257='parachute'\n)\n\nimagenette_label_func = imagenette[\"train\"].info.features[\"label\"].int2str\n\ndef imagenette_get_y(item):\n    label = imagenette_label_func(item[\"label\"])\n    return imagenette_label_dict[label]\n\n\n\nThe label dictionary was taken from fastai’s “Training Imagenette” tutorial.\nNow we’ll run the exact same pre-processing steps, create a DataBlock and DataLoaders, time the whole process with time.monotonic, and keep track of memory usage the %%memit magic function.\n\n%load_ext memory_profiler\n\n\nfrom fastai.vision.all import Resize, aug_transforms\n\n\n%%memit\n\nstart_time = time.monotonic()\n\n# Step 1\nnum_train, num_valid = len(imagenette[\"train\"]), len(imagenette[\"test\"])\nvalid_idxs = list(range(num_train, num_train + num_valid))\n\n# Step 2\nconcat_imagenette = concatenate_datasets([imagenette[\"train\"],\n                                          imagenette[\"test\"]])\n\n# Step 3\nimagenette_dblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_x=get_x,\n    get_y=imagenette_get_y,\n    splitter=IndexSplitter(valid_idx=valid_idxs),\n    item_tfms=[Resize(460)],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                *aug_transforms(size=224, min_scale=0.75),\n                RandomErasing()],\n)\n\n# instantiate the dataloaders\nimagenette_dls = imagenette_dblock.dataloaders(concat_imagenette, bs=64)\n\nend_time = time.monotonic()\nprint(\"Time elapsed:\", round(end_time - start_time, 1), \"seconds\")\n\nTime elapsed: 35.6 seconds\npeak memory: 22003.21 MiB, increment: 15222.07 MiB\n\n\nThat’s a big memory increment! It looks like we need a different approach to using Hugging Face image datasets with fastai.\n\n\nImage Paths\nThe Hugging Face datasets documentation indicates that, when loading a dataset that’s stored locally, we can get back paths instead of decoded images by using Dataset’s cast_column method. However, if you’re downloading a dataset from the Hugging Face Hub, as we’re doing in this post, you may get the images in a compressed format instead of as individual files, and this option won’t work.\n\nimagenette_with_paths = imagenette.cast_column(\"image\", datasets.Image(decode=False))\nlist(imagenette_with_paths[\"train\"][0][\"image\"].keys())\n\n['bytes', 'path']\n\n\n\nimagenette_with_paths[\"train\"][0][\"image\"][\"path\"] is None\n\nTrue\n\n\n\n\nThe Mid-Level API\nThe mid-level API has the same problem as the DataBlock API. Instantiating a fastai Datasets or TfmdLists object using Imagenette has the same problems as the DataBlock API, so we need to pass to fastai’s low-level API.\n\n\nThe Low-Level API\nA faster way to use a Hugging Face image dataset with fastai is via the low-level API: Pipelines of fastai Transforms and fastai’s custom DataLoader class, which is compatible with PyTorch DataLoaders but has some additional fastai-flavored features. We will also need to define some custom fastai-style transforms to handle our dataset format. Let’s start by working with Fashion-MNIST.\nWe’ll start with some data conversion transforms. Hugging Face datasets return indexed items as dictionaries…\n\nsample_input = imagenette[\"train\"][0]\nsample_input\n\n{'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x484&gt;,\n 'label': 7}\n\n\n… while fastai transforms expect tuples, so we’ll need a transform to convert formats. Instead of writing a __call__ method, fastai transforms need an encodes method.\n\nfrom fastai.vision.all import Transform\n\nclass DictToTuple(Transform):\n    order = -10 # do this before everything else\n    split_idx = None # apply to both training and validation sets\n\n    def encodes(self, x):\n        return x[\"image\"], x[\"label\"]\n\n\n\nWe’re relying on the fact that both of the datasets in this post use the column names image and label. A more flexible design would pass the relevant column names at instantiation or automatically extract them from the dataset if possible.\nLet’s make sure our transform does what we want it to do.\n\ndict_to_tuple = DictToTuple()\ntuple_input = dict_to_tuple(sample_input)\ntuple_input\n\n(&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x484&gt;, 7)\n\n\n\n\nInstead of calling the encodes method directly, we just use the instantiated Transform like a normal Python callable.\nNext, fastai’s image transforms work best with its custom PILImage class, so we’ll need another transform to convert our images to that format.\n\nfrom PIL import Image\nfrom fastai.vision.all import PILImage\n\nclass ConvertImage(Transform):\n    order = -9 # do this after DictToTuple\n    split_idx = None # apply to both training and validation sets\n\n    def encodes(self, x: Image.Image):\n        return PILImage.create(x)\n\nWhen a fastai Transform is passed a tuple as input, fastcore’s type dispatch system applies it to individual elements of the tuple that match the type annotation of the encodes method. For example, ConvertImage will only be applied to PIL.Image.Image objects, and our target integer labels will be left alone.\n\nconvert_image = ConvertImage()\nimage_converted = convert_image(tuple_input)\nimage_converted\n\n(PILImage mode=RGB size=320x484, 7)\n\n\nFinally, we’ll write a transform to convert the integer labels in our dataset to fastai TensorCategory labels, and we’ll make sure they know how to decode and display themselves in figures later on by writing a decodes method.\n\nfrom fastai.vision.all import TensorCategory\n\nclass ConvertCategory(Transform):\n    order = -8 # do this after ConvertImage\n    split_idx = None # apply to both the training and validation sets\n\n    def __init__(self, label_func):\n        self.label_func = label_func\n\n    def encodes(self, x: int):\n        return TensorCategory(x)\n\n    def decodes(self, x: TensorCategory):\n        return self.label_func(x)\n\nAs before, fastai’s type dispatching ensures that this transform will only be applied to the integer labels and will leave the images alone.\n\ndef label2cat(x):\n    decoded_label = imagenette_label_func(x.item())\n    return imagenette_label_dict[decoded_label]\n\nconvert_category = ConvertCategory(label2cat)\ncategory_converted = convert_category(image_converted)\ncategory_converted\n\n(PILImage mode=RGB size=320x484, TensorCategory(7))\n\n\nThe decodes method also uses type dispatching.\n\ncategory_decoded = convert_category.decode(category_converted)\ncategory_decoded\n\n(PILImage mode=RGB size=320x484, 'gas pump')\n\n\n\n\nInstead of calling the decodes method of the Transform directly, we call the decode method (without the s).\nI could not get fastai’s RandomErasing to work with the low-level API, so we’ll convert the implementation in timm to a fastai Transform.\n\n# make sure not to override fastai's RandomErasing class\nimport timm.data.random_erasing as random_erasing\n\n# fastai's ToTensor transform converts PILImages to fastai TensorImages\nfrom fastai.vision.all import TensorImage\n\nclass RandomErase(Transform):\n    order = 100 # do this after Normalize\n    split_idx = 0 # apply only to the training set\n\n    def __init__(self, p=0.5, mode='pixel', max_count=1):\n        device = ('cuda' if torch.cuda.is_available()\n                  else 'mps' if torch.backends.mps.is_available()\n                  else 'cpu')\n        self.tfm = random_erasing.RandomErasing(\n            probability=p, mode=mode, device=device, max_count=max_count\n        )\n\n    def encodes(self, x: TensorImage):\n        return self.tfm(x)\n\nWe’ll always want some specific transforms in our data pre-processing, but we might want to experiment with including or excluding others. To make that easier, we’ll design our data pipelines to be easily extensible. The transforms in a fastai Pipeline are automatically ordered by their order class attributes, which makes extensibility much easier to implement.\n\nfrom fastai.vision.all import ToTensor, IntToFloatTensor, Pipeline\n\ndef item_tfms_pipeline(split_idx, label_func, extra_tfms=None):\n    tfms = [DictToTuple(),\n            ConvertImage(),\n            ConvertCategory(label_func),\n            ToTensor()]\n    if extra_tfms is not None:\n        tfms.extend(extra_tfms)\n    return Pipeline(tfms, split_idx=split_idx)\n\ndef batch_tfms_pipeline(split_idx, extra_tfms=None):\n    tfms = [IntToFloatTensor()]\n    if extra_tfms is not None:\n        tfms.extend(extra_tfms)\n    return Pipeline(tfms, split_idx=split_idx)\n\nNow we can write some functions that generate a custom DataLoaders.\n\nfrom fastai.vision.all import DataLoader, DataLoaders\n\ndef get_dl(ds, bs, shuffle, device, label_func, item_tfms=None, batch_tfms=None):\n    return DataLoader(\n        ds, bs=bs, shuffle=shuffle,\n        after_item=item_tfms_pipeline(\n            split_idx=int(not shuffle),\n            label_func=label_func,\n            extra_tfms=item_tfms\n        ),\n        after_batch=batch_tfms_pipeline(\n            split_idx=int(not shuffle),\n            extra_tfms=batch_tfms\n        ),\n        device=device,\n        num_workers=8,\n    )\n\ndef get_dls(ds_dict, bs, item_tfms, batch_tfms, label_dict=None):\n    def label_func(x):\n        result = ds_dict[\"train\"].info.features[\"label\"].int2str(x)\n        if label_dict is not None:\n            result = [label_dict[o] for o in result]\n        return result\n\n    device = ('cuda' if torch.cuda.is_available()\n              else 'mps' if torch.backends.mps.is_available()\n              else 'cpu')\n\n    dls = DataLoaders(\n        *[get_dl(ds_dict[k], bs=bs, shuffle=(k==\"train\"), device=device,\n                 label_func=label_func, item_tfms=item_tfms, batch_tfms=batch_tfms)\n          for k in ds_dict],\n    )\n\n    # need to set this or vision_learner complains\n    dls.c = len(ds_dict[\"train\"].info.features[\"label\"].names)\n\n    return dls\n\nWe’ve designed things so that the syntax is familiar to what we used with DataBlocks.\n\nfashion_mnist_low_level_dls = get_dls(\n    ds_dict=fashion_mnist,\n    bs=512,\n    item_tfms=[CropPad(32),\n               RandomCrop(28)],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                Rotate(),\n                RandomErase()]\n)\n\nNow let’s train a model using our custom DataLoaders and compare with the DataBlock API.\n\nfrom fastai.vision.all import CrossEntropyLossFlat\n\nfashion_mnist_low_level_learn = vision_learner(\n    dls=fashion_mnist_low_level_dls,\n    arch=resnet18,\n    loss_func=CrossEntropyLossFlat(),\n    metrics=accuracy\n)\nlrs = fashion_mnist_low_level_learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs usual, we’ll use the learning rate given by the point marked steep.\n\nfashion_mnist_low_level_learn.fine_tune(3, lrs.steep)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.360952\n0.741448\n0.745417\n00:20\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.596736\n0.389657\n0.862917\n00:19\n\n\n1\n0.468772\n0.316965\n0.880583\n00:16\n\n\n2\n0.383638\n0.258381\n0.902500\n00:17\n\n\n\n\n\nThe training time per epoch is a little longer, but we still save time overall when we take into account the time needed to instantiate the DataLoaders. Let’s time the DataLoaders instantiation so we can compare the low-level API with the DataBlock API:\n\nstart_time = time.monotonic()\n\nfashion_mnist_low_level_dls = get_dls(\n    ds_dict=fashion_mnist,\n    bs=512,\n    item_tfms=[CropPad(32),\n               RandomCrop(28)],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                Rotate(),\n                RandomErase()]\n)\n\nend_time = time.monotonic()\n\nprint(\"Time elapsed:\", round(end_time - start_time, 3), \"seconds\")\n\nTime elapsed: 0.003 seconds\n\n\nMuch faster!\nThe fastai DataLoader class doesn’t have a built-in show_batch method, but we can easily patch one in with fastcore’s @patch decorator. There is already a show_batch function in fastai, and patch takes advantage of the type dispatch system mentioned above to add show_batch as a method to the DataLoaders class.\n\nfrom operator import itemgetter\nfrom fastcore.all import patch, mapt\nfrom fastai.vision.all import show_images\n\n@patch\ndef show_batch(self: DataLoaders, split='train', max_n=8, figsize=(6, 3)):\n    batch = getattr(self, split).one_batch()\n    slicer = itemgetter(slice(max_n))\n    batch = mapt(slicer, batch)\n    batch = getattr(self, split).after_batch.decode(batch)\n    batch = getattr(self, split).after_item.decode(batch)\n    images, labels = batch # unpacking for clarity\n    show_images(images, nrows=2, figsize=figsize, titles=labels)\n\nfashion_mnist_low_level_dls.show_batch()\n\n\n\n\n\n\n\n\n\n\nA fastai Pipeline has its own decode method that calls the decode methods of the constituent transforms in the appropriate reversed order. Each dataloader in a fastai DataLoaders has its own after_item and after_batch pipelines.\nNow let’s see how long it takes to instantiate a DataLoaders and train a model on Imagenette with the low-level API. We’ll need an extra custom Transform to convert the grayscale images in Imagenette to RGB format.\n\nclass ConvertToRGB(Transform):\n    order = 6 # after ToTensor\n    split_idx = None # apply to both the training and validation sets\n\n    def encodes(self, x: TensorImage):\n        if x.shape[0] == 3:\n            return x\n        return x.repeat(3, 1, 1)\n\nWe’ll also measure the memory usage when instantiating the DataLoaders.\n\n%%memit\n\nstart_time = time.monotonic()\n\nimagenette_low_level_dls = get_dls(\n    ds_dict=imagenette,\n    bs=64,\n    item_tfms=[Resize(460), ConvertToRGB()],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                *aug_transforms(size=224, min_scale=0.75),\n                RandomErase()],\n    label_dict=imagenette_label_dict\n)\n\nend_time = time.monotonic()\nprint(\"Elapsed time:\", round(end_time - start_time, 3), \"seconds\")\n\nElapsed time: 0.006 seconds\npeak memory: 22117.25 MiB, increment: 0.00 MiB\n\n\nMuch faster, and we’re not using any additional memory!\nWe can also look at a batch of Imagenette images using our patched-in show_batch method for DataLoaders.\n\nimagenette_low_level_dls.show_batch(figsize=(10, 5))\n\n\n\n\n\n\n\n\nNow let’s fine-tune a model.\n\nimagenette_low_level_learn = vision_learner(\n    dls=imagenette_low_level_dls,\n    arch=resnet18,\n    loss_func=CrossEntropyLossFlat(),\n    metrics=accuracy\n)\nlrs = imagenette_low_level_learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs usual, we’ll take the point marked steep as our learning rate.\n\nimagenette_low_level_learn.fine_tune(3, lrs.steep)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.334781\n0.070108\n0.979097\n00:23\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.197842\n0.070056\n0.980590\n00:24\n\n\n1\n0.143560\n0.058491\n0.983949\n00:23\n\n\n2\n0.101656\n0.048196\n0.986562\n00:24\n\n\n\n\n\nJust like with Fashion-MNIST, the training time per epoch for Imagenette is longer with the low-level API than with the DataBlock API. However, we save so much time and memory usage when creating the DataLoaders with the low-level API that it wins by default. Our data pipeline is also still quite flexible and extensible for working with Hugging Face datasets, so there’s not much downside."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Using Hugging Face Image Datasets with fastai\n\n\n\n\n\n\nComputer Vision\n\n\nHugging Face\n\n\n\nWe’ll look at different ways of using fastai’s data pipeline with Hugging Face image datasets and find an efficient option.\n\n\n\n\n\nFeb 12, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning an Image Dataset with ResNet Embeddings and fastai\n\n\n\n\n\n\nComputer Vision\n\n\n\nWe’ll clean up some problems in the Oxford-IIIT Pets dataset.\n\n\n\n\n\nJan 8, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\nNo matching items"
  }
]