[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jacob Rooney",
    "section": "",
    "text": "I’m a mathematician interested in machine learning.\n\nEducation\nUniversity of California, Los Angeles | Los Angeles, CA\nPh.D. in Mathematics | 2012—2018\nUniversity of California, Los Angeles | Los Angeles, CA\nM.S. in Mathematics | 2014\nUniversity of Georgia | Athens, GA\nB.S. in Mathematics | 2008—2012\n\n\nExperience\nSimons Center for Geometry and Physics | Stony Brook, NY\nResearch Assistant Professor (Postdoc) | 2020—2023\nUniversity of California, Los Angeles | Los Angeles, CA\nAssistant Adjunct Professor (Postdoc) | 2019—2020\nUniversity of Southern California | Los Angeles, CA\nAssistant Professor (RTPC) (Postdoc) | 2018—2019"
  },
  {
    "objectID": "blog/posts/2024-08-31-la-crime-dashboard/index.html",
    "href": "blog/posts/2024-08-31-la-crime-dashboard/index.html",
    "title": "A Dashboard for Crime in Los Angeles",
    "section": "",
    "text": "Click here for a full-screen version of the dashboard.\nA Jupyter notebook detailing how to download and process the data used in the dashboard is available here"
  },
  {
    "objectID": "blog/posts/2024-02-12-hf-datasets-with-fastai/index.html",
    "href": "blog/posts/2024-02-12-hf-datasets-with-fastai/index.html",
    "title": "Using Hugging Face Image Datasets with fastai",
    "section": "",
    "text": "Overview and Setup\nThe goal of this post is to find an efficient way of using fastai’s data pipeline tools when working with Hugging Face datasets. As it turns out, the DataBlock and mid-level APIs have some drawbacks in this context, and we’ll see that a better option is to use the low-level API: fastai’s custom DataLoader and Pipeline classes. If you just want to see how to use the low-level API, feel free to skip to that section.\n\nimport torch\nfrom datasets import load_dataset, load_dataset_builder\n\nThe list of Python packages and specific versions used to make this post are in the table below.\n\n\n\n\n\nPackage\nVersion\n\n\n\n\npython\n3.9.16\n\n\nPyTorch\n2.2.0+cu121\n\n\nfastai\n2.7.14\n\n\ntimm\n0.9.12\n\n\ndatasets\n2.15.0\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe latest version of the datasets library at time of writing is 2.16.1, but some of the dataset metadata was missing when using both versions 2.16.0 and 2.16.1.\n\n\n\n\nGet the Metadata\nWe’ll be using the Fashion-MNIST dataset in our analysis. It’s a drop-in replacement for the original MNIST dataset. We’ll use the version of Fashion-MNIST from the Hugging Face Hub. One nice feature of Hugging Face’s datasets library is the DatasetBuilder class that contains metadata for a given dataset. You can use load_dataset_builder to download a DatasetBuilder for any dataset on the Hugging Face Hub before downloading the dataset itself.\n\nfashion_mnist_builder = load_dataset_builder(\"fashion_mnist\")\n\nThe metadata we want is stored in the builder’s info attribute.\n\nfashion_mnist_info = fashion_mnist_builder.info\n\nFor example, there’s a written description of the dataset…\n\nprint(fashion_mnist_info.description)\n\nFashion-MNIST is a dataset of Zalando's article images—consisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.\n\n\n\n… a dictionary of pre-defined splits in the dataset, with relevant metadata…\n\nfashion_mnist_info.splits\n\n{'train': SplitInfo(name='train', num_bytes=31296607, num_examples=60000, shard_lengths=None, dataset_name='fashion_mnist'),\n 'test': SplitInfo(name='test', num_bytes=5233810, num_examples=10000, shard_lengths=None, dataset_name='fashion_mnist')}\n\n\n… and a dictionary of all of the columns in the dataset, with their datatypes and relevant metadata.\n\nfashion_mnist_info.features\n\n{'image': Image(decode=True, id=None),\n 'label': ClassLabel(names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\nThe ClassLabel object in info.features has a convenient int2str method to convert integer labels to the strings that they represent.\n\nfashion_mnist_label_func = fashion_mnist_info.features[\"label\"].int2str\nnum_classes = len(fashion_mnist_info.features[\"label\"].names)\n{i: fashion_mnist_label_func(i) for i in range(num_classes)}\n\n{0: 'T - shirt / top',\n 1: 'Trouser',\n 2: 'Pullover',\n 3: 'Dress',\n 4: 'Coat',\n 5: 'Sandal',\n 6: 'Shirt',\n 7: 'Sneaker',\n 8: 'Bag',\n 9: 'Ankle boot'}\n\n\n\n\nGet the Data\nWe can also download and load the dataset using our DatasetBuilder.\n\nfashion_mnist_builder.download_and_prepare()\nfashion_mnist = fashion_mnist_builder.as_dataset()\nfashion_mnist\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\nThe splits, features, and number of training and test instances all match the metadata that we looked at in the previous section. We’ll discard the provided test set and divide the training set into a new training and validation set using the Hugging Face Dataset class’s built-in train_test_split method.\n\nfashion_mnist = fashion_mnist[\"train\"].train_test_split(test_size=0.2)\nfashion_mnist\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 48000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 12000\n    })\n})\n\n\n\n\nThe DataBlock API\nOne commonly used data processing API in the fastai library is the DataBlock API. We’ll need to do some additional pre-processing steps when working with a Hugging Face Dataset. The procedure we use is taken from an example in the documentation for blurr, a library that makes it easy to use Hugging Face transformers with fastai. The specific example I’m referencing is the second example in the linked “Prepare the datasets” section.\nStep 1: Record the indices of the validation set.\n\nnum_train, num_valid = len(fashion_mnist[\"train\"]), len(fashion_mnist[\"test\"])\nvalid_idxs = list(range(num_train, num_train + num_valid))\n\nStep 2: Concatenate the training and validation sets into one dataset.\nWe’ll use the concatenate_datasets function from the datasets library.\n\nfrom datasets import concatenate_datasets\n\nconcat_fashion_mnist = concatenate_datasets([fashion_mnist[\"train\"],\n                                             fashion_mnist[\"test\"]])\n\nStep 3: Pass an IndexSplitter when creating our DataBlock so that it knows which instances go in the training and validation sets.\n\nfrom fastai.vision.all import (\n    DataBlock, ImageBlock, CategoryBlock, IndexSplitter, CropPad,\n    RandomCrop, Normalize, imagenet_stats, Rotate, RandomErasing\n)\n\ndef get_x(item):\n    return item[\"image\"]\n\ndef get_y(item):\n    return fashion_mnist_label_func(item[\"label\"])\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_x=get_x,\n    get_y=get_y,\n    splitter=IndexSplitter(valid_idx=valid_idxs),\n    item_tfms=[CropPad(32),\n               RandomCrop(28)],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                Rotate(),\n                RandomErasing()],\n)\n\nNow we can create our DataLoaders.\n\nfashion_mnist_dls = dblock.dataloaders(concat_fashion_mnist, bs=512)\n\nAs usual, we can use fastai’s conveniences like show_batch.\n\nfashion_mnist_dls.show_batch(figsize=(5, 5))\n\n\n\n\n\n\n\n\nThen we can create a Learner and fine-tune a model on our dataset like normal.\n\nfrom fastai.vision.all import (\n    vision_learner, resnet18, accuracy,\n    minimum, steep, valley, slide\n)\n\nfashion_mnist_learn = vision_learner(fashion_mnist_dls, resnet18, metrics=accuracy)\nlrs = fashion_mnist_learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph seems to be steepest at the point marked steep, so we’ll take that as our learning rate.\n\nfashion_mnist_learn.fine_tune(3, lrs.steep)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.324347\n0.824672\n0.700250\n00:15\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.608801\n0.476613\n0.848417\n00:12\n\n\n1\n0.468581\n0.308588\n0.882750\n00:11\n\n\n2\n0.400869\n0.285220\n0.893000\n00:11\n\n\n\n\n\nGreat!\n\n\nThe Downside of the DataBlock API with a Hugging Face Dataset\nEverything looks fine on the surface, but there’s a hidden cost of using the DataBlock API with a Hugging Face Dataset: the time it takes to create the DataLoaders. Let’s re-run our DataBlock pipeline and time it.\n\nimport time\n\nstart_time = time.monotonic()\n\n# Step 1\nnum_train, num_valid = len(fashion_mnist[\"train\"]), len(fashion_mnist[\"test\"])\nvalid_idxs = list(range(num_train, num_train + num_valid))\n\n# Step 2\nconcat_fashion_mnist = concatenate_datasets([fashion_mnist[\"train\"],\n                                             fashion_mnist[\"test\"]])\n\n# Step 3\nfashion_mnist_dblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_x=get_x,\n    get_y=get_y,\n    splitter=IndexSplitter(valid_idx=valid_idxs),\n    item_tfms=[CropPad(32),\n               RandomCrop(28)],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                Rotate(),\n                RandomErasing()],\n)\n\n# instantiate the dataloaders\nfashion_mnist_dls = fashion_mnist_dblock.dataloaders(concat_fashion_mnist, bs=512)\n\nend_time = time.monotonic()\nprint(\"Time elapsed:\", round(end_time - start_time, 1), \"seconds\")\n\nTime elapsed: 18.9 seconds\n\n\nThat’s too long for such a small dataset, and if we create a DataBlock with a bigger dataset like Imagenette, the time required to instantiate the DataLoaders will be even longer. Let’s first download the dataset…\n\nimagenette = load_dataset(\"johnowhitaker/imagenette2-320\")\n\n\n\nMany thanks to Jonathan Whitaker for uploading this version of Imagenette to the Hugging Face Hub!\n… then split the provided training set into a training and validation set…\n\nimagenette = imagenette[\"train\"].train_test_split(test_size=0.2)\nimagenette\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10715\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 2679\n    })\n})\n\n\n… then set up some new functions to use in our DataBlock.\n\nimagenette_label_dict = dict(\n    n01440764='tench',\n    n02102040='English springer',\n    n02979186='cassette player',\n    n03000684='chain saw',\n    n03028079='church',\n    n03394916='French horn',\n    n03417042='garbage truck',\n    n03425413='gas pump',\n    n03445777='golf ball',\n    n03888257='parachute'\n)\n\nimagenette_label_func = imagenette[\"train\"].info.features[\"label\"].int2str\n\ndef imagenette_get_y(item):\n    label = imagenette_label_func(item[\"label\"])\n    return imagenette_label_dict[label]\n\n\n\nThe label dictionary was taken from fastai’s “Training Imagenette” tutorial.\nNow we’ll run the exact same pre-processing steps, create a DataBlock and DataLoaders, time the whole process with time.monotonic, and keep track of memory usage the %%memit magic function.\n\n%load_ext memory_profiler\n\n\nfrom fastai.vision.all import Resize, aug_transforms\n\n\n%%memit\n\nstart_time = time.monotonic()\n\n# Step 1\nnum_train, num_valid = len(imagenette[\"train\"]), len(imagenette[\"test\"])\nvalid_idxs = list(range(num_train, num_train + num_valid))\n\n# Step 2\nconcat_imagenette = concatenate_datasets([imagenette[\"train\"],\n                                          imagenette[\"test\"]])\n\n# Step 3\nimagenette_dblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_x=get_x,\n    get_y=imagenette_get_y,\n    splitter=IndexSplitter(valid_idx=valid_idxs),\n    item_tfms=[Resize(460)],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                *aug_transforms(size=224, min_scale=0.75),\n                RandomErasing()],\n)\n\n# instantiate the dataloaders\nimagenette_dls = imagenette_dblock.dataloaders(concat_imagenette, bs=64)\n\nend_time = time.monotonic()\nprint(\"Time elapsed:\", round(end_time - start_time, 1), \"seconds\")\n\nTime elapsed: 35.6 seconds\npeak memory: 22003.21 MiB, increment: 15222.07 MiB\n\n\nThat’s a big memory increment! It looks like we need a different approach to using Hugging Face image datasets with fastai.\n\n\nImage Paths\nThe Hugging Face datasets documentation indicates that, when loading a dataset that’s stored locally, we can get back paths instead of decoded images by using Dataset’s cast_column method. However, if you’re downloading a dataset from the Hugging Face Hub, as we’re doing in this post, you may get the images in a compressed format instead of as individual files, and this option won’t work.\n\nimagenette_with_paths = imagenette.cast_column(\"image\", datasets.Image(decode=False))\nlist(imagenette_with_paths[\"train\"][0][\"image\"].keys())\n\n['bytes', 'path']\n\n\n\nimagenette_with_paths[\"train\"][0][\"image\"][\"path\"] is None\n\nTrue\n\n\n\n\nThe Mid-Level API\nThe mid-level API has the same problem as the DataBlock API. Instantiating a fastai Datasets or TfmdLists object using Imagenette has the same problems as the DataBlock API, so we need to pass to fastai’s low-level API.\n\n\nThe Low-Level API\nA faster way to use a Hugging Face image dataset with fastai is via the low-level API: Pipelines of fastai Transforms and fastai’s custom DataLoader class, which is compatible with PyTorch DataLoaders but has some additional fastai-flavored features. We will also need to define some custom fastai-style transforms to handle our dataset format. Let’s start by working with Fashion-MNIST.\nWe’ll start with some data conversion transforms. Hugging Face datasets return indexed items as dictionaries…\n\nsample_input = imagenette[\"train\"][0]\nsample_input\n\n{'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x484&gt;,\n 'label': 7}\n\n\n… while fastai transforms expect tuples, so we’ll need a transform to convert formats. Instead of writing a __call__ method, fastai transforms need an encodes method.\n\nfrom fastai.vision.all import Transform\n\nclass DictToTuple(Transform):\n    order = -10 # do this before everything else\n    split_idx = None # apply to both training and validation sets\n\n    def encodes(self, x):\n        return x[\"image\"], x[\"label\"]\n\n\n\nWe’re relying on the fact that both of the datasets in this post use the column names image and label. A more flexible design would pass the relevant column names at instantiation or automatically extract them from the dataset if possible.\nLet’s make sure our transform does what we want it to do.\n\ndict_to_tuple = DictToTuple()\ntuple_input = dict_to_tuple(sample_input)\ntuple_input\n\n(&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x484&gt;, 7)\n\n\n\n\nInstead of calling the encodes method directly, we just use the instantiated Transform like a normal Python callable.\nNext, fastai’s image transforms work best with its custom PILImage class, so we’ll need another transform to convert our images to that format.\n\nfrom PIL import Image\nfrom fastai.vision.all import PILImage\n\nclass ConvertImage(Transform):\n    order = -9 # do this after DictToTuple\n    split_idx = None # apply to both training and validation sets\n\n    def encodes(self, x: Image.Image):\n        return PILImage.create(x)\n\nWhen a fastai Transform is passed a tuple as input, fastcore’s type dispatch system applies it to individual elements of the tuple that match the type annotation of the encodes method. For example, ConvertImage will only be applied to PIL.Image.Image objects, and our target integer labels will be left alone.\n\nconvert_image = ConvertImage()\nimage_converted = convert_image(tuple_input)\nimage_converted\n\n(PILImage mode=RGB size=320x484, 7)\n\n\nFinally, we’ll write a transform to convert the integer labels in our dataset to fastai TensorCategory labels, and we’ll make sure they know how to decode and display themselves in figures later on by writing a decodes method.\n\nfrom fastai.vision.all import TensorCategory\n\nclass ConvertCategory(Transform):\n    order = -8 # do this after ConvertImage\n    split_idx = None # apply to both the training and validation sets\n\n    def __init__(self, label_func):\n        self.label_func = label_func\n\n    def encodes(self, x: int):\n        return TensorCategory(x)\n\n    def decodes(self, x: TensorCategory):\n        return self.label_func(x)\n\nAs before, fastai’s type dispatching ensures that this transform will only be applied to the integer labels and will leave the images alone.\n\ndef label2cat(x):\n    decoded_label = imagenette_label_func(x.item())\n    return imagenette_label_dict[decoded_label]\n\nconvert_category = ConvertCategory(label2cat)\ncategory_converted = convert_category(image_converted)\ncategory_converted\n\n(PILImage mode=RGB size=320x484, TensorCategory(7))\n\n\nThe decodes method also uses type dispatching.\n\ncategory_decoded = convert_category.decode(category_converted)\ncategory_decoded\n\n(PILImage mode=RGB size=320x484, 'gas pump')\n\n\n\n\nInstead of calling the decodes method of the Transform directly, we call the decode method (without the s).\nI could not get fastai’s RandomErasing to work with the low-level API, so we’ll convert the implementation in timm to a fastai Transform.\n\n# make sure not to override fastai's RandomErasing class\nimport timm.data.random_erasing as random_erasing\n\n# fastai's ToTensor transform converts PILImages to fastai TensorImages\nfrom fastai.vision.all import TensorImage\n\nclass RandomErase(Transform):\n    order = 100 # do this after Normalize\n    split_idx = 0 # apply only to the training set\n\n    def __init__(self, p=0.5, mode='pixel', max_count=1):\n        device = ('cuda' if torch.cuda.is_available()\n                  else 'mps' if torch.backends.mps.is_available()\n                  else 'cpu')\n        self.tfm = random_erasing.RandomErasing(\n            probability=p, mode=mode, device=device, max_count=max_count\n        )\n\n    def encodes(self, x: TensorImage):\n        return self.tfm(x)\n\nWe’ll always want some specific transforms in our data pre-processing, but we might want to experiment with including or excluding others. To make that easier, we’ll design our data pipelines to be easily extensible. The transforms in a fastai Pipeline are automatically ordered by their order class attributes, which makes extensibility much easier to implement.\n\nfrom fastai.vision.all import ToTensor, IntToFloatTensor, Pipeline\n\ndef item_tfms_pipeline(split_idx, label_func, extra_tfms=None):\n    tfms = [DictToTuple(),\n            ConvertImage(),\n            ConvertCategory(label_func),\n            ToTensor()]\n    if extra_tfms is not None:\n        tfms.extend(extra_tfms)\n    return Pipeline(tfms, split_idx=split_idx)\n\ndef batch_tfms_pipeline(split_idx, extra_tfms=None):\n    tfms = [IntToFloatTensor()]\n    if extra_tfms is not None:\n        tfms.extend(extra_tfms)\n    return Pipeline(tfms, split_idx=split_idx)\n\nNow we can write some functions that generate a custom DataLoaders.\n\nfrom fastai.vision.all import DataLoader, DataLoaders\n\ndef get_dl(ds, bs, shuffle, device, label_func, item_tfms=None, batch_tfms=None):\n    return DataLoader(\n        ds, bs=bs, shuffle=shuffle,\n        after_item=item_tfms_pipeline(\n            split_idx=int(not shuffle),\n            label_func=label_func,\n            extra_tfms=item_tfms\n        ),\n        after_batch=batch_tfms_pipeline(\n            split_idx=int(not shuffle),\n            extra_tfms=batch_tfms\n        ),\n        device=device,\n        num_workers=8,\n    )\n\ndef get_dls(ds_dict, bs, item_tfms, batch_tfms, label_dict=None):\n    def label_func(x):\n        result = ds_dict[\"train\"].info.features[\"label\"].int2str(x)\n        if label_dict is not None:\n            result = [label_dict[o] for o in result]\n        return result\n\n    device = ('cuda' if torch.cuda.is_available()\n              else 'mps' if torch.backends.mps.is_available()\n              else 'cpu')\n\n    dls = DataLoaders(\n        *[get_dl(ds_dict[k], bs=bs, shuffle=(k==\"train\"), device=device,\n                 label_func=label_func, item_tfms=item_tfms, batch_tfms=batch_tfms)\n          for k in ds_dict],\n    )\n\n    # need to set this or vision_learner complains\n    dls.c = len(ds_dict[\"train\"].info.features[\"label\"].names)\n\n    return dls\n\nWe’ve designed things so that the syntax is familiar to what we used with DataBlocks.\n\nfashion_mnist_low_level_dls = get_dls(\n    ds_dict=fashion_mnist,\n    bs=512,\n    item_tfms=[CropPad(32),\n               RandomCrop(28)],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                Rotate(),\n                RandomErase()]\n)\n\nNow let’s train a model using our custom DataLoaders and compare with the DataBlock API.\n\nfrom fastai.vision.all import CrossEntropyLossFlat\n\nfashion_mnist_low_level_learn = vision_learner(\n    dls=fashion_mnist_low_level_dls,\n    arch=resnet18,\n    loss_func=CrossEntropyLossFlat(),\n    metrics=accuracy\n)\nlrs = fashion_mnist_low_level_learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs usual, we’ll use the learning rate given by the point marked steep.\n\nfashion_mnist_low_level_learn.fine_tune(3, lrs.steep)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.360952\n0.741448\n0.745417\n00:20\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.596736\n0.389657\n0.862917\n00:19\n\n\n1\n0.468772\n0.316965\n0.880583\n00:16\n\n\n2\n0.383638\n0.258381\n0.902500\n00:17\n\n\n\n\n\nThe training time per epoch is a little longer, but we still save time overall when we take into account the time needed to instantiate the DataLoaders. Let’s time the DataLoaders instantiation so we can compare the low-level API with the DataBlock API:\n\nstart_time = time.monotonic()\n\nfashion_mnist_low_level_dls = get_dls(\n    ds_dict=fashion_mnist,\n    bs=512,\n    item_tfms=[CropPad(32),\n               RandomCrop(28)],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                Rotate(),\n                RandomErase()]\n)\n\nend_time = time.monotonic()\n\nprint(\"Time elapsed:\", round(end_time - start_time, 3), \"seconds\")\n\nTime elapsed: 0.003 seconds\n\n\nMuch faster!\nThe fastai DataLoader class doesn’t have a built-in show_batch method, but we can easily patch one in with fastcore’s @patch decorator. There is already a show_batch function in fastai, and patch takes advantage of the type dispatch system mentioned above to add show_batch as a method to the DataLoaders class.\n\nfrom operator import itemgetter\nfrom fastcore.all import patch, mapt\nfrom fastai.vision.all import show_images\n\n@patch\ndef show_batch(self: DataLoaders, split='train', max_n=8, figsize=(6, 3)):\n    batch = getattr(self, split).one_batch()\n    slicer = itemgetter(slice(max_n))\n    batch = mapt(slicer, batch)\n    batch = getattr(self, split).after_batch.decode(batch)\n    batch = getattr(self, split).after_item.decode(batch)\n    images, labels = batch # unpacking for clarity\n    show_images(images, nrows=2, figsize=figsize, titles=labels)\n\nfashion_mnist_low_level_dls.show_batch()\n\n\n\n\n\n\n\n\n\n\nA fastai Pipeline has its own decode method that calls the decode methods of the constituent transforms in the appropriate reversed order. Each dataloader in a fastai DataLoaders has its own after_item and after_batch pipelines.\nNow let’s see how long it takes to instantiate a DataLoaders and train a model on Imagenette with the low-level API. We’ll need an extra custom Transform to convert the grayscale images in Imagenette to RGB format.\n\nclass ConvertToRGB(Transform):\n    order = 6 # after ToTensor\n    split_idx = None # apply to both the training and validation sets\n\n    def encodes(self, x: TensorImage):\n        if x.shape[0] == 3:\n            return x\n        return x.repeat(3, 1, 1)\n\nWe’ll also measure the memory usage when instantiating the DataLoaders.\n\n%%memit\n\nstart_time = time.monotonic()\n\nimagenette_low_level_dls = get_dls(\n    ds_dict=imagenette,\n    bs=64,\n    item_tfms=[Resize(460), ConvertToRGB()],\n    batch_tfms=[Normalize.from_stats(*imagenet_stats),\n                *aug_transforms(size=224, min_scale=0.75),\n                RandomErase()],\n    label_dict=imagenette_label_dict\n)\n\nend_time = time.monotonic()\nprint(\"Elapsed time:\", round(end_time - start_time, 3), \"seconds\")\n\nElapsed time: 0.006 seconds\npeak memory: 22117.25 MiB, increment: 0.00 MiB\n\n\nMuch faster, and we’re not using any additional memory!\nWe can also look at a batch of Imagenette images using our patched-in show_batch method for DataLoaders.\n\nimagenette_low_level_dls.show_batch(figsize=(10, 5))\n\n\n\n\n\n\n\n\nNow let’s fine-tune a model.\n\nimagenette_low_level_learn = vision_learner(\n    dls=imagenette_low_level_dls,\n    arch=resnet18,\n    loss_func=CrossEntropyLossFlat(),\n    metrics=accuracy\n)\nlrs = imagenette_low_level_learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs usual, we’ll take the point marked steep as our learning rate.\n\nimagenette_low_level_learn.fine_tune(3, lrs.steep)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.334781\n0.070108\n0.979097\n00:23\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.197842\n0.070056\n0.980590\n00:24\n\n\n1\n0.143560\n0.058491\n0.983949\n00:23\n\n\n2\n0.101656\n0.048196\n0.986562\n00:24\n\n\n\n\n\nJust like with Fashion-MNIST, the training time per epoch for Imagenette is longer with the low-level API than with the DataBlock API. However, we save so much time and memory usage when creating the DataLoaders with the low-level API that it wins by default. Our data pipeline is also still quite flexible and extensible for working with Hugging Face datasets, so there’s not much downside."
  },
  {
    "objectID": "blog/posts/2024-04-11-diffedit/index.html",
    "href": "blog/posts/2024-04-11-diffedit/index.html",
    "title": "Implementing DiffEdit: Editing Images with Text Prompts",
    "section": "",
    "text": "Overview and Setup\nThe goal of this post is to implement DiffEdit, an algorithm that edits images using text prompts, proposed in the paper DiffEdit: Diffusion-based semantic image editing with mask guidance. The following pair of images illustrates what the algorithm can do. The image on the left is a real image from the USDA’s SNAP-Ed page on blackberries. The image on the right was generated from the real image using DiffEdit and the indicated text prompts.\n\nThanks go to Hugging Face for its wonderful diffusers library used in this post and for its tutorials on how to use the library, which helped me a great deal. Thanks also go to Jonathan Whitaker for his very helpful deep dive notebook on Stable Diffusion, which I used as a source when creating this post. Finally, thanks go to the authors of the DiffEdit paper, Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord, for developing the algorithm and for the clear writing and explanations in their paper.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\n\nimport torch\nimport torchvision.transforms as tfms\nfrom transformers import (\n    CLIPTokenizer,\n    CLIPTextModel,\n)\nfrom diffusers import (\n    DDPMScheduler,\n    AutoencoderKL,\n    UNet2DConditionModel\n)\n\n\n\n\n\n\nPackage\nVersion\n\n\n\n\npython\n3.9.16\n\n\nPyTorch\n2.2.2+cu121\n\n\ntorchvision\n0.17.2+cu121\n\n\nmatplotlib\n3.8.4\n\n\nnumpy\n1.26.4\n\n\nPIL\n10.3.0\n\n\ntransformers\n4.39.3\n\n\ndiffusers\n0.27.2\n\n\n\n\n\n\n\nOutline of DiffEdit\nThe DiffEdit algorithm has three steps.\n\nStep 1: Generate a Mask\n\nGenerate a binary mask highlighting the portion of the image that we want to edit.\n\nStep 2: Add Noise\n\nAdd noise to the image using an inverse DDIM process as described in the paper Denoising Diffusion Implicit Models.\n\nStep 3: Carefully Remove the Noise\n\nRemove the noise from the final noised image using a DDIM process as described in the same paper above, modified to make the edit that we want.\n\n\n\n\nStep 1: Generate a Mask\nFirst, let’s load our image.\n\nfrom diffusers.utils import load_image\n\nimage_path = \"usda_images/blackberries.jpg\"\nsample_image = load_image(image_path)\nsample_image\n\n\n\n\n\n\n\n\nWe’ll generate our mask by adding noise to the image and then estimating the noise in two ways: once conditioned on a text prompt that accurately describes the image and once conditioned on a text prompt that reflects the edit we want to make. First, let’s load all of the models we’ll need.\n\nvariational auto-encoder\nscheduler\ntokenizer\ntext encoder\nU-Net\n\nThese components have all been pre-trained to work well together.\n\ndevice = (\n    \"cuda\" if torch.cuda.is_available()\n    else \"mps\" if torch.backends.mps.is_available()\n    else \"cpu\"\n)\n\ncheckpoint = \"CompVis/stable-diffusion-v1-4\"\n\nvae = AutoencoderKL.from_pretrained(\n    checkpoint, subfolder=\"vae\", use_safetensors=True,\n).to(device)\n\nscheduler = DDPMScheduler.from_pretrained(\n    checkpoint, subfolder=\"scheduler\",\n)\n\ntokenizer = CLIPTokenizer.from_pretrained(\n    checkpoint, subfolder=\"tokenizer\",\n)\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    checkpoint, subfolder=\"text_encoder\", use_safetensors=True,\n).to(device)\n\nunet = UNet2DConditionModel.from_pretrained(\n    checkpoint, subfolder=\"unet\", use_safetensors=True,\n).to(device)\n\nBefore we do anything, let’s set the number of timesteps we want to use for our DDIM and inverse DDIM processes. With the scheduler we’re using, the number of timesteps can be anywhere from \\(1\\) to \\(1000\\). The more timesteps we use, the better our outputs will be, but the longer it will take to generate those outputs. We’ll use \\(100\\) timesteps, which will generate good results without taking too long.\n\nscheduler.set_timesteps(100)\n\nThe editing process doesn’t happen on the original image. Instead, we work with a compressed version of the image called its latent representation. The latent image is much smaller than the original, so the editing process is much faster. The following utility functions convert back and forth between an image and its latent representation.\n\ndef image2latent(image, vae):\n    image = tfms.ToTensor()(image)[None].to(device)\n    with torch.no_grad():\n        latent = vae.encode(image * 2 - 1)\n    return 0.18215 * latent.latent_dist.sample()\n\ndef latent2image(latent, vae):\n    latent = latent / 0.18215\n    with torch.no_grad():\n        image = vae.decode(latent).sample\n    image = (image + 1) / 2\n    image = image.clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    image = (image * 255).round().astype(np.uint8)\n    image = Image.fromarray(image[0])\n    return image\n\nWe’ll add noise to the image (really, to its latent representation) using our scheduler’s add_noise() method.\n\ndef add_noise_at_strength(latent, scheduler, strength=0.5):\n    num_timesteps = len(scheduler.timesteps)\n    target_timestep = num_timesteps - int(num_timesteps * strength)\n    noise = torch.randn_like(latent)\n    noisy_latent = scheduler.add_noise(\n        latent, noise, scheduler.timesteps[target_timestep][None]\n    )\n    return noisy_latent, target_timestep\n\nThe strength parameter in the function can vary from \\(0\\) to \\(1\\) and controls how much noise we add. Setting it to \\(0\\) adds no noise, while setting it to \\(1\\) replaces the entire latent with pure noise. The DiffEdit paper recommends adding noise at strength \\(0.5\\). Let’s see what that looks like when we convert the noised latent back to an image.\n\nsample_latent = image2latent(sample_image, vae)\nsample_noisy_latent, sample_timestep = add_noise_at_strength(\n    sample_latent, scheduler, strength=0.5\n)\nlatent2image(sample_noisy_latent, vae)\n\n\n\n\n\n\n\n\nNow we’ll define all of the other functions we need to generate the mask. We need to\n\nembed the text prompts\npredict the noise\npredict the difference in noise, averaged over 10 attempts for stability\ngenerate the mask itself\n\n\ndef embed_prompt(prompt, tokenizer, text_encoder):\n    # tokenize the input\n    text_input = tokenizer(\n        prompt,\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    ).to(device)\n\n    # convert the tokenized text to embeddings\n    with torch.no_grad():\n        text_embedding = text_encoder(\n            text_input.input_ids\n        ).last_hidden_state\n    \n    # create the \"unconditional embedding\"\n    max_length = text_input.input_ids.shape[-1] \n    unconditional_input = tokenizer(\n        [\"\"],\n        padding=\"max_length\",\n        max_length=max_length,\n        return_tensors=\"pt\"\n    ).to(device)\n    with torch.no_grad():\n        unconditional_embedding = text_encoder(\n            unconditional_input.input_ids\n        ).last_hidden_state\n    \n    # concatenate the embeddings for classifier-free guidance\n    text_embedding = torch.cat(\n        [unconditional_embedding, text_embedding]\n    )\n\n    return text_embedding\n\ndef predict_noise(\n        latent,\n        embedded_prompt,\n        scheduler,\n        timestep,\n        unet,\n        guidance_scale=7.5,\n    ):\n    # predict the noise\n    t = scheduler.timesteps[timestep]\n    latent_model_input = torch.cat([latent] * 2)\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n    with torch.no_grad():\n        noise_pred = unet(\n            latent_model_input, t, encoder_hidden_states=embedded_prompt\n        )[\"sample\"]\n    \n    # do classifier-free guidance\n    noise_pred_unconditional, noise_pred_prompt = noise_pred.chunk(2)\n    noise_pred = (\n        noise_pred_unconditional\n        + guidance_scale * (noise_pred_prompt - noise_pred_unconditional)\n    )\n\n    return noise_pred\n\ndef compute_average_noise_difference(\n        latent,\n        embedded_prompt_1,\n        embedded_prompt_2,\n        scheduler,\n        timestep,\n        unet,\n        guidance_scale=7.5,\n        num_iterations=10,\n    ):\n    result = []\n    for _ in range(num_iterations):\n        # predict noise for both prompts\n        noise_1 = predict_noise(\n            latent, embedded_prompt_1, scheduler, timestep, unet, guidance_scale\n        )\n        noise_2 = predict_noise(\n            latent, embedded_prompt_2, scheduler, timestep, unet, guidance_scale\n        )\n\n        # compute the average absolute difference over latent channels\n        noise_diff = (noise_1 - noise_2).abs().mean(dim=1)\n\n        # remove extreme noise predictions\n        upper_bound = noise_diff.mean() + noise_diff.std()\n        noise_diff = noise_diff.clamp_max(upper_bound)\n\n        result.append(noise_diff)\n    \n    # average the absolute differences for stability\n    result = torch.cat(result).mean(dim=0)\n\n    # rescale to the interval [0, 1]\n    result = (result - result.min()) / (result.max() - result.min())\n\n    return result\n\ndef generate_mask(\n        image,\n        vae,\n        prompt_1,\n        prompt_2,\n        scheduler,\n        tokenizer,\n        text_encoder,\n        unet,\n        num_iterations=10,\n        guidance_scale=7.5,\n        strength=0.5,\n    ):\n    # convert image to YUV format to do histogram equalization\n    # not in DiffEdit paper, but helps with objects in shadows\n    image = np.array(image)\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n\n    # do histogram equalization\n    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n    image[..., 0] = clahe.apply(image[..., 0])\n\n    # convert image back to RGB format\n    image = cv2.cvtColor(image, cv2.COLOR_YUV2RGB)\n    image = Image.fromarray(image)\n\n    # add noise\n    latent = image2latent(image, vae)\n    latent, timestep = add_noise_at_strength(latent, scheduler, strength=strength)\n\n    # compute the mask\n    embedded_prompt_1 = embed_prompt(prompt_1, tokenizer, text_encoder)\n    embedded_prompt_2 = embed_prompt(prompt_2, tokenizer, text_encoder)\n    noise_diff = compute_average_noise_difference(\n        latent,\n        embedded_prompt_1,\n        embedded_prompt_2,\n        scheduler,\n        timestep,\n        unet,\n        guidance_scale,\n        num_iterations\n    )\n    mask = postprocess_and_binarize(noise_diff)\n\n    return mask\n\ndef postprocess_and_binarize(noise_diff, filter_size=15, threshold=0.3):\n    # apply Gaussian blur to improve the quality of the mask\n    noise_diff = noise_diff.cpu().numpy()\n    noise_diff = cv2.GaussianBlur(noise_diff, (filter_size, filter_size), 0)\n    \n    # binarize and put on GPU\n    mask = torch.from_numpy(noise_diff &gt; threshold).long()\n    mask = mask.to(device)\n\n    return mask\n\nNow we’ll finally generate our mask. Following the DiffEdit paper, we’ll call the prompt that accurately descsribes the image the reference text and the prompt that reflects the edits we want to make the query.\n\nreference_text = \"A bowl of blackberries.\"\nquery = \"A bowl of blueberries.\"\n\nsample_mask = generate_mask(\n    sample_image,\n    vae,\n    reference_text,\n    query,\n    scheduler,\n    tokenizer,\n    text_encoder,\n    unet,\n    guidance_scale=7.5,\n    strength=0.5,\n    num_iterations=10,\n)\n\nLet’s see how we did.\n\nresized_mask = (sample_mask.cpu().numpy().astype(np.uint8)) * 255\nresized_mask = Image.fromarray(resized_mask)\nresized_mask = resized_mask.resize(sample_image.size)\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\naxs[0].imshow(sample_image)\naxs[1].imshow(sample_image)\naxs[1].imshow(resized_mask, cmap=\"Greys_r\", alpha=0.5)\naxs[0].set_axis_off()\naxs[1].set_axis_off()\naxs[0].set_title(\"Original Image\")\naxs[1].set_title(\"Image with Mask Overlay\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStep 2: Add Noise\nNow we’ll add noise to our image with an inverse DDIM process. We’ll also save the intermediate noised latents for use in Step 3, where they’ll help keep the edited image close to the original outside of the mask.\nThe inverse DDIM process uses the following equation (Equation 2 in the DiffEdit paper). I’ve changed the notation slightly to make it clear that it can be used to both add and remove noise.\n\\[ \\mathbf x_{t + \\Delta t} = \\sqrt{\\alpha_{t + \\Delta t}} \\left( \\frac{\\mathbf x_t - \\sqrt{1 - \\alpha_t} \\epsilon_\\theta(\\mathbf x_t, t)}{\\sqrt{\\alpha_t}} \\right) + \\sqrt{1 - \\alpha_{t + \\Delta t}} \\; \\epsilon_\\theta(\\mathbf x_t, t)\\]\nHere, \\(t\\) is a timestep in our inverse DDIM process, which we view as the current timestep, \\(t + \\Delta t\\) is the next timestep in the process, \\(\\mathbf x_t\\) is the partially noised latent at the current timestep, \\(x_{t + \\Delta t}\\) is the partially noised latent that we want to calculate for the next timestep, \\(\\alpha_t\\) is a coefficient that represents the amount of noise that should be in the latent at timestep \\(t\\), and \\(\\epsilon_\\theta(\\mathbf x_t, t)\\) is the estimated noise in the latent at timestep \\(t\\).\nThe timesteps \\(t\\) and coefficients \\(\\alpha_t\\) are built into our scheduler object. We’ll compute the predicted noise \\(\\epsilon_\\theta\\) using our pre-trained U-Net.\nNow let’s re-write the equation to simplify it a bit. First, we can rearrange it to get\n\\[ \\frac{\\mathbf x_{t + \\Delta t}}{\\sqrt{\\alpha_{t + \\Delta t}}} = \\frac{\\mathbf x_t}{\\sqrt{\\alpha_t}} + \\left( \\sqrt{\\frac{1 - \\alpha_{t + \\Delta t}}{\\alpha_{t + \\Delta t}}} - \\sqrt{\\frac{1 - \\alpha_t}{\\alpha_t}} \\right) \\epsilon_\\theta(\\mathbf x_t, t).\\]\nThen we can set \\(\\beta_t = 1 - \\alpha_t\\) to get\n\\[ \\frac{\\mathbf x_{t + \\Delta t}}{\\sqrt{\\alpha_{t + \\Delta t}}} = \\frac{\\mathbf x_t}{\\sqrt{\\alpha_t}} + \\left( \\sqrt{\\frac{\\beta_{t + \\Delta t}}{\\alpha_{t + \\Delta t}}} - \\sqrt{\\frac{\\beta_t}{\\alpha_t}} \\right) \\epsilon_\\theta(\\mathbf x_t, t).\\]\nFinally, we can set \\(\\gamma_t = \\sqrt{\\beta_t / \\alpha_t}\\) and multiply both sides by \\(\\sqrt{\\alpha_{t + \\Delta t}}\\) to get\n\\[ \\mathbf x_{t + \\Delta t} = \\sqrt{\\frac{\\alpha_{t + \\Delta t}}{\\alpha_t}} \\, \\mathbf x_t + \\sqrt{\\alpha_{t + \\Delta t}} \\left( \\gamma_{t + \\Delta t} - \\gamma_t \\right) \\epsilon_\\theta(\\mathbf x_t, t),\\]\nwhich is the equation we’ll use below.\nIn the inverse DDIM process, \\(\\gamma_{t + \\Delta t} - \\gamma_t\\) will be positive, so we’ll add noise to the latent at every step. In the regular DDIM process, the timesteps are reversed and \\(\\gamma_{t + \\Delta t} - \\gamma_t\\) will be negative, so we’ll remove noise from the latent at every step.\n\ndef ddim_prep_timesteps(scheduler, encoding_ratio=0.5):\n    timesteps = scheduler.timesteps\n    num_timesteps = len(timesteps)\n    target_timestep = num_timesteps - int(num_timesteps * encoding_ratio)\n    timesteps = timesteps[target_timestep:]\n    return timesteps\n\ndef ddim_noising(\n        latent,\n        scheduler,\n        unet,\n        tokenizer,\n        text_encoder,\n        encoding_ratio=0.5,\n        guidance_scale=7.5\n    ):\n    results = []\n\n    # embed the unconditional prompt and prep the timesteps\n    embedding = embed_prompt(\"\", tokenizer, text_encoder)\n    timesteps = ddim_prep_timesteps(scheduler, encoding_ratio=encoding_ratio)\n    timesteps = reversed(timesteps)\n\n    # prep the alphas and assemble a generator of consecutive pairs\n    alphas = scheduler.alphas_cumprod[timesteps]\n    alphas = torch.cat([alphas.new_tensor([1.0]), alphas])\n    alpha_pairs = zip(alphas[:-1], alphas[1:])\n\n    for t, (alpha_t, alpha_t_plus_delta_t) in zip(timesteps, alpha_pairs):\n        # compute the unconditional noise\n        latent_model_input = torch.cat([latent] * 2)\n        with torch.no_grad():\n            noise_pred = unet(\n                latent_model_input, t, encoder_hidden_states=embedding\n            ).sample\n        noise_pred_unconditional, noise_pred_prompt = noise_pred.chunk(2)\n        noise_pred = (\n            noise_pred_unconditional\n            + guidance_scale * (noise_pred_prompt - noise_pred_unconditional)\n        )\n\n        # compute the coefficients in the DDIM equation\n        beta_t = 1 - alpha_t\n        beta_t_plus_delta_t = 1 - alpha_t_plus_delta_t\n        gamma_t = (beta_t / alpha_t).sqrt()\n        gamma_t_plus_delta_t = (beta_t_plus_delta_t / alpha_t_plus_delta_t).sqrt()\n\n        # add noise to the latent using the DDIM equation\n        latent = (\n            (alpha_t_plus_delta_t / alpha_t).sqrt() * latent\n            + alpha_t_plus_delta_t.sqrt() * (gamma_t_plus_delta_t - gamma_t) * noise_pred\n        )\n\n        results.append(latent)\n\n    return results\n\n\nsample_noised_latents = ddim_noising(\n    sample_latent,\n    scheduler,\n    unet,\n    tokenizer,\n    text_encoder,\n    encoding_ratio=0.8,\n    guidance_scale=7.5,\n)\n\nLet’s see what some of these partially noised latents look like.\n\nlatents_to_show = sample_noised_latents[9::10]\nlatents_to_show = [sample_latent] + latents_to_show\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(14, 10))\n\nfor noised_latent, ax in zip(latents_to_show, axs.flat):\n    noised_image = latent2image(noised_latent, vae)\n    ax.imshow(noised_image)\n    ax.set_axis_off()\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStep 3: Carefully Remove the Noise\nNow we’ll remove the noise from our saved noised latents while conditioning on the query, which at each step will push the portion of the image highlighted by the mask towards the edit that we want. We’ll use the same equation for the DDIM process that we used for the inverse process, just reversing the sequence of timesteps \\(t\\). At each step, we’ll also replace the pixels not highlighted by the mask with the pixels from the corresponding partially noised image from Step 2, reducing the chance that the algorithm makes stray edits to the image.\n\ndef ddim_guided_denoising(\n        noised_latents,\n        mask,\n        prompt,\n        scheduler,\n        unet,\n        tokenizer,\n        text_encoder,\n        encoding_ratio=0.5,\n        guidance_scale=7.5,\n    ):\n    # prep the list of partially noised latents\n    # we'll need the original latent without noise for the last step\n    noised_latents = list(reversed(noised_latents)) + [sample_latent]\n\n    # we start with the most noised latent\n    latent = noised_latents[0]\n\n    # embed the query and prep the timesteps\n    embedding = embed_prompt(prompt, tokenizer, text_encoder)\n    timesteps = ddim_prep_timesteps(scheduler, encoding_ratio=encoding_ratio)\n\n    # prep the alphas and assemble a generator of consecutive pairs\n    alphas = scheduler.alphas_cumprod[timesteps]\n    alphas = torch.cat([alphas, alphas.new_tensor([1.0])])\n    alpha_pairs = zip(alphas[:-1], alphas[1:])\n\n    loop_data = zip(timesteps, noised_latents, alpha_pairs)\n    for t, prev_latent, (alpha_t, alpha_t_plus_delta_t) in loop_data:\n        # compute the noise conditioned on the query\n        latent_model_input = torch.cat([latent] * 2)\n        with torch.no_grad():\n            noise_pred = unet(\n                latent_model_input, t, encoder_hidden_states=embedding\n            ).sample\n        noise_pred_unconditional, noise_pred_prompt = noise_pred.chunk(2)\n        noise_pred = (\n            noise_pred_unconditional\n            + guidance_scale * (noise_pred_prompt - noise_pred_unconditional)\n        )\n\n        # compute the coefficients in the DDIM equation\n        beta_t = 1 - alpha_t\n        beta_t_plus_delta_t = 1 - alpha_t_plus_delta_t\n        gamma_t = (beta_t / alpha_t).sqrt()\n        gamma_t_plus_delta_t = (beta_t_plus_delta_t / alpha_t_plus_delta_t).sqrt()\n\n        # remove noise from the latent using the DDIM equation\n        latent = (\n            (alpha_t_plus_delta_t / alpha_t).sqrt() * latent\n            + alpha_t_plus_delta_t.sqrt() * (gamma_t_plus_delta_t - gamma_t) * noise_pred\n        )\n\n        # replace pixels outside of the mask with those from the previous noised latent\n        # helps limit stray edits to the original image\n        latent = mask * latent + (1 - mask) * prev_latent\n\n    return latent\n\n\nedited_sample_latent = ddim_guided_denoising(\n    sample_noised_latents,\n    sample_mask,\n    query,\n    scheduler,\n    unet,\n    tokenizer,\n    text_encoder,\n    encoding_ratio=0.8,\n    guidance_scale=7.5,\n)\n\nLet’s see how we did.\n\nedited_sample_image = latent2image(edited_sample_latent, vae)\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\naxs[0].imshow(sample_image)\naxs[1].imshow(edited_sample_image)\naxs[0].set_axis_off()\naxs[1].set_axis_off()\naxs[0].set_title(reference_text)\naxs[1].set_title(query)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe’ve recovered the figure from the beginning of the post!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacob Rooney",
    "section": "",
    "text": "I’m a mathematician interested in machine learning.\n \n  \n   \n  \n    \n     GitHub"
  },
  {
    "objectID": "index.html#a-little-about-me",
    "href": "index.html#a-little-about-me",
    "title": "Jacob Rooney",
    "section": "A Little About Me",
    "text": "A Little About Me\nAfter getting a PhD in mathematics at UCLA, I spent several years in academia before becoming interested in machine learning. In particular, I want to see what practical insights and advancements mathematics can bring to the machine learning world. You can read more detailed information about me here."
  },
  {
    "objectID": "index.html#recent-blog-posts",
    "href": "index.html#recent-blog-posts",
    "title": "Jacob Rooney",
    "section": "Recent Blog Posts",
    "text": "Recent Blog Posts\nI recently started to blog about the projects I’m working on. My most recent ones are listed below, and you can check out all of them here.\n\n\n\n\n\n\n\n\n\n\nA Dashboard for Crime in Los Angeles\n\n\n\n\n\nA Shiny dashboard for crimes reported and arrests made in Los Angeles from 2020 through 2024.\n\n\n\n\n\nAug 31, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing DiffEdit: Editing Images with Text Prompts\n\n\n\n\n\nWe’ll implement DiffEdit by hand and show what it can do.\n\n\n\n\n\nApr 11, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding and Training a RetinaNet\n\n\n\n\n\nWe’ll build a RetinaNet model and train it on PASCAL VOC.\n\n\n\n\n\nMar 25, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Hugging Face Image Datasets with fastai\n\n\n\n\n\nWe’ll look at different ways of using fastai’s data pipeline with Hugging Face image datasets and find an efficient option.\n\n\n\n\n\nFeb 12, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning an Image Dataset with ResNet Embeddings and fastai\n\n\n\n\n\nWe’ll clean up some problems in the Oxford-IIIT Pets dataset.\n\n\n\n\n\nJan 8, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2024-03-25-object-detection/index.html",
    "href": "blog/posts/2024-03-25-object-detection/index.html",
    "title": "Building and Training a RetinaNet",
    "section": "",
    "text": "Overview and Setup\nThe goal of this post is to build a RetinaNet model and train it on the PASCAL VOC dataset. We’ll use a couple of tricks, including fine-tuning the RetinaNet’s backbone on a related task, to speed up training and get results in about 2 hours total. Our trained model will predict bounding boxes for 20 classes of objects in images. For example, the model we train in this post makes the following predictions for the image below. The decimal numbers in each box label is the model’s confidence score for that label.\n\n\n\n\n\n\nFigure 1: An example input image and predicted bounding boxes from the model trained in this post. The input image was taken from this KerasCV tutorial\n\n\n\n\nimport albumentations as A\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport os\n\n\n\n\n\n\nPackage\nVersion\n\n\n\n\npython\n3.9.16\n\n\nPyTorch\n2.2.1+cu121\n\n\ntorchvision\n0.17.1+cu121\n\n\nfastai\n2.7.14\n\n\nmatplotlib\n3.8.3\n\n\nnumpy\n1.25.2\n\n\nalbumentations\n1.4.2\n\n\npycocotools\n2.0.7\n\n\n\n\n\n\n\nGet and Format the Data\nWe’ll use fastai’s versions of the PASCAL VOC datasets. We’ll combine the 2007 and 2012 training and validation sets to use as our training set, and we’ll use the 2007 test set as our validation set.\n\nfrom fastai.vision.all import untar_data, URLs\n\npath_2007 = untar_data(URLs.PASCAL_2007)\nos.listdir(path_2007)\n\n['test.json',\n 'test.csv',\n 'segmentation',\n 'valid.json',\n 'train',\n 'test',\n 'train.csv',\n 'train.json']\n\n\n\npath_2012 = untar_data(URLs.PASCAL_2012)\nos.listdir(path_2012)\n\n['segmentation', 'valid.json', 'train', 'test', 'train.csv', 'train.json']\n\n\n\ntrain_data_sources = [\n    path_2007/'train.json',\n    path_2007/'valid.json',\n    path_2012/'train.json',\n    path_2012/'valid.json',\n]\nvalid_data_source = path_2007/'test.json'\n\nThe annotation JSON files are in the COCO format, and fastai has a convenience get_annotations function to extract the data relevant for our task. To speed up training the RetinaNet, we’ll first fine-tune the ResNet backbone on a multilabel classification task. For organizational efficiency, we’ll generate and store the targets for both the classification and object detection tasks together and extract the relevant targets for each task when needed with custom Datasets and DataLoader collation functions.\n\nfrom fastai.vision.all import get_annotations\nfrom itertools import chain, starmap\n\ndef get_vocab_dicts(data_source):\n    _, targets = get_annotations(data_source)\n    all_labels = set().union(*(set(o[1]) for o in targets))\n    all_labels = sorted(all_labels)\n    idx2label = dict(enumerate(all_labels))\n    label2idx = {v: k for k, v in idx2label.items()}\n    return idx2label, label2idx\n\ndef organize_annotations(train_data_sources, valid_data_source):\n    train_images, train_targets = [\n        list(chain.from_iterable(o))\n        for o in zip(*[get_annotations(source, prefix=str(source.parent)+'/train/')\n                       for source in train_data_sources])\n    ]\n    valid_images, valid_targets = get_annotations(\n        valid_data_source, prefix=str(valid_data_source.parent)+'/test/'\n    )\n    _, label2idx = get_vocab_dicts(train_data_sources[0])\n    return (\n        (train_images, train_targets, label2idx),\n        (valid_images, valid_targets, label2idx)\n    )\n\ndef reformat_data(imgs, targs, label2idx):\n    result = [\n        {'image': img,\n         'bboxes': targ[0],\n         'labels': [label2idx[o] for o in targ[1]]}\n        for img, targ in zip(imgs, targs)\n    ]\n    for item in result:\n        multilabel = [0] * len(label2idx)\n        for i in set(item['labels']):\n            multilabel[i] = 1\n        item |= {'multilabel': multilabel}\n    return result\n\ndef organize_data(train_data_sources, valid_data_source):\n    results = organize_annotations(\n        train_data_sources, valid_data_source\n    )\n    return starmap(reformat_data, results)\n\ntrain_data, valid_data = organize_data(\n    train_data_sources, valid_data_source\n)\n\nNow let’s take a look at how we’ve formatted the data.\n\ntrain_data[1]\n\n{'image': '/root/.fastai/data/pascal_2007/train/000017.jpg',\n 'bboxes': [[184, 61, 279, 199], [89, 77, 403, 336]],\n 'labels': [14, 12],\n 'multilabel': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]}\n\n\nFor later use, we’ll generate dictionaries to go back and forth between integer and string class labels.\n\nidx2label, label2idx = get_vocab_dicts(train_data_sources[0])\nidx2label\n\n{0: 'aeroplane',\n 1: 'bicycle',\n 2: 'bird',\n 3: 'boat',\n 4: 'bottle',\n 5: 'bus',\n 6: 'car',\n 7: 'cat',\n 8: 'chair',\n 9: 'cow',\n 10: 'diningtable',\n 11: 'dog',\n 12: 'horse',\n 13: 'motorbike',\n 14: 'person',\n 15: 'pottedplant',\n 16: 'sheep',\n 17: 'sofa',\n 18: 'train',\n 19: 'tvmonitor'}\n\n\n\n\nVisualize the Data\nLet’s take a look at some of the images in our dataset together with their bounding box annotations.\n\n\nImage Plotting Functions\nimport matplotlib.patheffects as path_effects\nfrom fastai.vision.all import PILImage, Path\nfrom PIL import Image\n\n\ncmap = matplotlib.colormaps['tab20']\nimagenet_stats = {\n    'mean': [0.485, 0.456, 0.406],\n    'std': [0.229, 0.224, 0.225],\n}\n\n\ndef draw_bbox(ax, bbox, label, alpha=1.0):\n    x0, y0, x1, y1 = bbox\n    box = matplotlib.patches.Rectangle(\n        (x0, y0), x1 - x0, y1 - y0,\n        facecolor='none',\n        linewidth=2,\n        edgecolor=cmap(label),\n        alpha=alpha,\n    )\n    border_effects = [\n        path_effects.Stroke(linewidth=5, foreground='black', alpha=alpha),\n        path_effects.Normal()\n    ]\n    box.set_path_effects(border_effects)\n    ax.add_patch(box)\n\n\ndef add_bbox_label(ax, bbox, label, alpha=1.0):\n    props = dict(\n        boxstyle='square',\n        facecolor=cmap(label),\n        edgecolor='black',\n        linewidth=1.1,\n        alpha=alpha\n    )\n    x0, y0, _, _ = bbox\n    if isinstance(label, torch.Tensor):\n        label = label.item()\n    ax.text(\n        x0, y0, idx2label[label],\n        bbox=props,\n        color='black',\n        fontsize=8,\n        in_layout=True\n    )\n\n\ndef decode(image):\n    if isinstance(image, Path):\n        image = Image.open(image)\n    if isinstance(image, torch.Tensor):\n        if image.shape[0] &lt;= 3:\n            image = image.permute(1, 2, 0)\n        image = image.cpu().numpy()\n    if isinstance(image, np.ndarray):\n        if image.dtype == np.float32:\n            mean = np.array(imagenet_stats['mean'])[None, None]\n            std = np.array(imagenet_stats['std'])[None, None]\n            image = (image*std + mean) * 255.0\n            image = image.astype(np.uint8)\n        image = Image.fromarray(image)\n    return image\n\n\ndef plot_image_with_annotations(image, bboxes, labels, ax=None, alpha=1.0, **kwargs):\n    if isinstance(image, str):\n        image = PILImage.create(image)\n    image = decode(image)\n    if isinstance(bboxes, torch.Tensor):\n        bboxes = bboxes.cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.cpu().numpy()\n\n    if ax is None:\n        fig, ax = plt.subplots()\n        fig.tight_layout()\n    ax.imshow(image)\n    ax.axis('off')\n\n    for bbox, label in zip(bboxes, labels):\n        draw_bbox(ax, bbox, label, alpha=alpha)\n        add_bbox_label(ax, bbox, label, alpha=alpha)\n\n\n\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\nfor idx, ax in enumerate(axs.flat):\n    plot_image_with_annotations(**train_data[idx], ax=ax)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nMulti-Label Classification\nOur first task is to fine-tune a ResNet to do multi-label classification on our dataset. As mentioned above, we’ll use a custom Dataset to extract only the targets that we need for this task.\n\ndef_device = ('cuda' if torch.cuda.is_available()\n              else 'mps' if torch.backends.mps.is_available()\n              else 'cpu')\n\nclass MultilabelDataset(torch.utils.data.Dataset):\n    def __init__(self, items, tfms=None, device=def_device):\n        self.items = items\n        self.tfms = tfms\n        self.device = device\n    \n    def __len__(self):\n        return len(self.items)\n    \n    def __getitem__(self, idx):\n        item = self.items[idx]\n        image_as_array = {'image': self.open_as_array(item['image'])}\n        item = item | image_as_array\n        item = {k: item[k] for k in ['image', 'multilabel']}\n        if self.tfms is not None:\n            item = item | self.tfms(image=item['image'])\n        return {k: torch.tensor(item[k]).float()\n                for k in ['image', 'multilabel']}\n    \n    def open_as_array(self, image):\n        return np.array(Image.open(image))\n\nNow we’ll set up our image transforms and create our training and validation datasets for this task. We’ll pad all of the images to be \\(600 \\times 600\\) to make them a uniform size while maintaining the aspect ratios of the bounding boxes in our later object detection model. We’ll also use minimal data augmentation: just random flips, since that’s all we’ll use for the object detection model.\n\nimage_size = 600\n\nimagenet_stats = {\n    'mean': [0.485, 0.456, 0.406],\n    'std': [0.229, 0.224, 0.225],\n}\n\npad_params = {\n    'min_height': image_size,\n    'min_width': image_size,\n    'position': 'top_left',\n    'border_mode': 0,\n    'value': 0,\n}\n\nmultilabel_train_tfms = A.Compose(\n    transforms=[\n        A.Flip(p=0.5),\n        A.PadIfNeeded(**pad_params),\n        A.Normalize(**imagenet_stats),\n    ]\n)\n\nmultilabel_valid_tfms = A.Compose(\n    transforms=[\n        A.PadIfNeeded(**pad_params),\n        A.Normalize(**imagenet_stats),\n    ]\n)\n\nmultilabel_train_ds = MultilabelDataset(\n    train_data, tfms=multilabel_train_tfms\n)\n\nmultilabel_valid_ds = MultilabelDataset(\n    valid_data, tfms=multilabel_valid_tfms\n)\n\n\n\nPadding the images in this way was inspired by this KerasCV tutorial.\nNow let’s visualize our transformed data.\n\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(16, 20))\nfor idx, ax in enumerate(axs.flat):\n    item = multilabel_train_ds[idx]\n    image, target = item['image'], item['multilabel']\n    title = ', '.join([idx2label[i.item()] for i in target.nonzero()])\n    ax.imshow(decode(image))\n    ax.set(xticks=[], yticks=[], title=title)\nfig.tight_layout()\n\n\n\n\n\n\n\n\nWe’ll use a custom collation function put our images in channels-first format.\n\nfrom fastai.vision.all import DataLoader, DataLoaders\n\ndef multilabel_collate(batch):\n    images = torch.stack([o['image'].permute(2, 0, 1) for o in batch])\n    multilabels = torch.stack([o['multilabel'] for o in batch])\n    return images, multilabels\n\nmultilabel_config = {\n    'bs': 24,\n    'create_batch': multilabel_collate,\n    'device': def_device,\n    'num_workers': 8\n}\n\nmultilabel_train_dl = DataLoader(\n    multilabel_train_ds, shuffle=True, **multilabel_config\n)\nmultilabel_valid_dl = DataLoader(\n    multilabel_valid_ds, shuffle=False, **multilabel_config\n)\n\nmultilabel_dls = DataLoaders(multilabel_train_dl, multilabel_valid_dl)\n\nNow we’ll define our multi-label image classification model. One of the tricks we’ll use to get better object detection results later on is to use Mish as our activation function instead of ReLU.\n\nfrom fastai.vision.all import create_body, create_head, resnet101\n\nclass MultilabelModel(nn.Module):\n    def __init__(self, n_out):\n        super().__init__()\n\n        self.backbone = create_body(resnet101(weights='DEFAULT'))\n        self.head = create_head(2048, n_out)\n\n        self.backbone = self.swap_activation(self.backbone)\n        self.head = self.swap_activation(self.head)\n    \n    def forward(self, x):\n        x = self.backbone(x)\n        return self.head(x)\n    \n    def swap_activation(self, module, old_act=nn.ReLU, new_act=nn.Mish):\n        for name, submodule in module._modules.items():\n            if len(list(submodule.children())) &gt; 0:\n                module._modules[name] = self.swap_activation(submodule)\n            if isinstance(submodule, old_act):\n                module._modules[name] = new_act(inplace=True)\n        return module\n\n\n\nThe code for swap_activation was adapted from this notebook by Radek Osmulski.\nNow we can find a good learning rate and fine-tune our multi-label classification model.\n\nfrom fastai.vision.all import (\n    params, L, accuracy_multi, Learner,\n    minimum, steep, valley, slide\n)\n\ndef multilabel_split(model):\n    return L(\n        model.backbone[:6],\n        model.backbone[6:],\n        model.head\n    ).map(params)\n\nmultilabel_learn = Learner(\n    dls=multilabel_dls,\n    model=MultilabelModel(20),\n    loss_func=nn.BCEWithLogitsLoss(),\n    metrics=accuracy_multi,\n    splitter=multilabel_split\n)\n\nmultilabel_learn.freeze()\nmultilabel_lrs = multilabel_learn.lr_find(\n    suggest_funcs=(minimum, steep, valley, slide)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmultilabel_learn.fine_tune(3, multilabel_lrs.slide)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.121236\n0.070566\n0.975909\n06:49\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.107960\n0.075465\n0.974031\n08:07\n\n\n1\n0.081165\n0.059422\n0.979069\n08:07\n\n\n2\n0.051065\n0.047460\n0.983330\n08:07\n\n\n\n\n\n\n\nObject Detection\nNow we’ll build a RetinaNet model using our fine-tuned ResNet as the backbone. We’ll need a new dataset class since our targets for object detection are different.\n\nclass ObjectDetectionDataset(torch.utils.data.Dataset):\n    def __init__(self, items, tfms=None, device=def_device):\n        self.items = items\n        self.tfms = tfms\n        self.device = device\n    \n    def __len__(self):\n        return len(self.items)\n    \n    def __getitem__(self, idx):\n        item = self.items[idx]\n        image_as_array = {'image': self.open_as_array(item['image'])}\n        item = item | image_as_array\n        item = {k: item[k] for k in ['image', 'bboxes', 'labels']}\n        if self.tfms is not None:\n            item = self.tfms(**item)\n        return {\n            k: torch.tensor(item[k]).to(t)\n            for k, t in [\n                ['image', torch.float32],\n                ['bboxes', torch.float32],\n                ['labels', torch.int64]\n            ]\n        }\n\n    def open_as_array(self, image):\n        return np.array(Image.open(image))\n\nNow we’ll set up the data processing pipeline for our object detection model. We’ll re-use the image_size, imagenet_stats, and pad_params from the multi-label classification model.\n\nbbox_params = {\n    'format': 'pascal_voc',\n    'min_visibility': 0.2,\n    'label_fields': ['labels']\n}\n\nobject_detection_train_tfms = A.Compose(\n    transforms=[\n        A.BBoxSafeRandomCrop(),\n        A.Flip(p=0.5),\n        A.PadIfNeeded(**pad_params),\n        A.Normalize(**imagenet_stats),\n    ], bbox_params=A.BboxParams(**bbox_params)\n)\n\nobject_detection_valid_tfms = A.Compose(\n    transforms=[\n        A.PadIfNeeded(**pad_params),\n        A.Normalize(**imagenet_stats),\n    ], bbox_params=A.BboxParams(**bbox_params)\n)\n\ntrain_ds = ObjectDetectionDataset(\n    train_data, tfms=object_detection_train_tfms\n)\n\nvalid_ds = ObjectDetectionDataset(\n    valid_data, tfms=object_detection_valid_tfms\n)\n\nLet’s visualize our object detection data with the transforms above.\n\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\nfor idx, ax in enumerate(axs.flat):\n    plot_image_with_annotations(**train_ds[idx], ax=ax)\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe number of bounding boxes and labels varies from image to image, so we’ll need to pad those targets when collating each batch.\n\ndef pad_bboxes(bboxes, max_targets):\n    pad_size = max_targets - len(bboxes)\n    padding = torch.tensor([[-100] * 4] * pad_size, device=bboxes.device)\n    return torch.cat([bboxes, padding])\n\ndef pad_labels(labels, max_targets):\n    pad_size = max_targets - len(labels)\n    padding = torch.tensor([-100] * pad_size, device=labels.device)\n    return torch.cat([labels, padding])\n\ndef object_detection_collate(batch):\n    images = torch.stack([o['image'].permute(2, 0, 1) for o in batch])\n    max_targets = max(len(o['labels']) for o in batch)\n    bboxes = torch.stack([pad_bboxes(o['bboxes'], max_targets) for o in batch])\n    labels = torch.stack([pad_labels(o['labels'], max_targets) for o in batch])\n    return images, bboxes, labels\n\nobject_detection_config = {\n    'bs': 8,\n    'create_batch': object_detection_collate,\n    'device': def_device,\n    'num_workers': 8\n}\n\nobject_detection_train_dl = DataLoader(\n    train_ds, shuffle=True, **object_detection_config\n)\n\nobject_detection_valid_dl = DataLoader(\n    valid_ds, shuffle=False, **object_detection_config\n)\n\nNow we’ll define our object detection model.\n\nfrom fastai.vision.all import hook_outputs\nfrom functools import partial\nimport math\n\ndef conv_with_init(\n        n_in,\n        n_out,\n        kernel_size=3,\n        stride=1,\n        bias=True,\n        weight_init=nn.init.kaiming_normal_,\n        bias_init=partial(nn.init.constant_, val=0.0)\n    ):\n    conv = nn.Conv2d(n_in, n_out, kernel_size=kernel_size,\n                     stride=stride, padding=kernel_size//2, bias=bias)\n    weight_init(conv.weight)\n    bias_init(conv.bias)\n    return conv\n\nclass ObjectDetectionHead(nn.Module):\n    def __init__(self, n_out, n_anchors, bias_init_val):\n        super().__init__()\n        self.n_out = n_out\n        layers = []\n        for _ in range(4):\n            layers += [conv_with_init(256, 256),\n                       nn.Mish()]\n        layers += [conv_with_init(\n            256, n_out * n_anchors,\n            weight_init=partial(nn.init.constant_, val=0.0),\n            bias_init=partial(nn.init.constant_, val=bias_init_val)\n        )]\n        self.head = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return torch.cat(\n            [self.reshape(self.head(o), self.n_out) for o in x], dim=1\n        )\n    \n    def reshape(self, x, n_out):\n        return (x.permute(0, 2, 3, 1)\n                 .reshape(x.shape[0], -1, n_out))\n\nclass ObjectDetectionModel(nn.Module):\n    def __init__(self, n_anchors, backbone=None):\n        super().__init__()\n        self.backbone = create_body(resnet101()) if backbone is None else backbone\n\n        # FPN top path\n        self.c5top5 = conv_with_init(2048, 256, kernel_size=1)\n        self.c5top6 = conv_with_init(2048, 256, stride=2)\n        self.p6top7 = nn.Sequential(nn.Mish(),\n                                    conv_with_init(256, 256, stride=2))\n\n        # FPN down path\n        self.c4_cross = conv_with_init(1024, 256, kernel_size=1)\n        self.c3_cross = conv_with_init(512, 256, kernel_size=1)\n\n        # smooth results of FPN down path\n        self.p3_out = conv_with_init(256, 256)\n        self.p4_out = conv_with_init(256, 256)\n        self.p5_out = conv_with_init(256, 256)\n\n        # bounding box regression head and image classification head\n        self.box_head = ObjectDetectionHead(4, n_anchors, 0.0)\n        prior = -math.log((1 - 0.01) / 0.01)\n        self.class_head = ObjectDetectionHead(20, n_anchors, prior)\n    \n    def forward(self, x):\n        hook_layers = self.backbone[-3:-1]\n        with hook_outputs(hook_layers, detach=False) as h:\n            c5 = self.backbone(x)\n        c3, c4 = h.stored\n\n        # FPN top path\n        p5 = self.c5top5(c5)\n        p6 = self.c5top6(c5)\n        p7 = self.p6top7(p6)\n\n        # FPN down path\n        p4 = self.c4_cross(c4) + F.interpolate(p5, size=38, mode='nearest-exact')\n        p3 = self.c3_cross(c3) + F.interpolate(p4, size=75, mode='nearest-exact')\n        \n        # smooth results of FPN down path\n        p3 = self.p3_out(p3)\n        p4 = self.p4_out(p4)\n        p5 = self.p5_out(p5)\n\n        fpn_out = [p3, p4, p5, p6, p7]\n        return self.box_head(fpn_out), self.class_head(fpn_out)\n\n\n\nThe code for the model is partially based on code from this Keras tutorial and this notebook from an old version of the fastai course.\nNow we’ll define our loss function, starting with code to generate anchor boxes.\n\nfrom torchvision.ops import box_convert, box_iou\nfrom itertools import product\n\ndef generate_centers(grid_size):\n    coords = torch.arange(0, grid_size, dtype=torch.float32, device=def_device) + 0.5\n    centers = torch.stack(torch.meshgrid(coords, coords.clone(), indexing='xy'), dim=-1)\n    return centers\n\ndef generate_one_anchor_grid(\n        image_size,\n        grid_size,\n        area,\n        aspect_ratio,\n        scale\n    ):\n    height = math.sqrt(area / aspect_ratio)\n    width = area / height\n    centers = generate_centers(grid_size) * (image_size / grid_size)\n    heights = torch.full((grid_size, grid_size, 1), scale * height, device=def_device)\n    widths = torch.full((grid_size, grid_size, 1), scale * width, device=def_device)\n    return torch.cat([centers, widths, heights], dim=-1)\n\ndef generate_anchor_grids(\n        image_size,\n        grid_size,\n        area,\n        aspect_ratios=None,\n        scales=None\n    ):\n    if aspect_ratios is None:\n        aspect_ratios = [1/2, 1, 2]\n    if scales is None:\n        scales = [math.pow(2, i / 3) for i in range(3)]\n    anchors = torch.empty(\n        grid_size, grid_size, len(aspect_ratios) * len(scales), 4,\n        dtype=torch.float32, device=def_device\n    )\n    for i, (r, s) in enumerate(product(aspect_ratios, scales)):\n        anchors[..., i, :] = generate_one_anchor_grid(\n            image_size, grid_size, area, r, s\n        )\n    return anchors.view(-1, 4)\n\ndef generate_anchor_boxes(\n        image_size,\n        areas=None,\n        grid_sizes=None,\n        aspect_ratios=None,\n        scales=None\n    ):\n    if grid_sizes is None:\n        grid_sizes = [75, 38, 19, 10, 5]\n    if areas is None:\n        areas = [(4 * image_size / grid_size)**2 for grid_size in grid_sizes]\n    return torch.cat(\n        [generate_anchor_grids(image_size, grid_size, area, aspect_ratios, scales)\n         for area, grid_size in zip(areas, grid_sizes)]\n    )\n\n\n\nThe code to generate anchor boxes is partially based on code from this Keras tutorial and this notebook from an old version of the fastai course.\nFor the loss function itself, we’ll use complete iou loss for the bounding box targets and focal loss for the classification targets.\n\nfrom torchvision.ops import sigmoid_focal_loss, complete_box_iou_loss\n\nclass ObjectDetectionLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.anchors = generate_anchor_boxes(600)\n        self.box_loss_func = complete_box_iou_loss\n        self.class_loss_func = sigmoid_focal_loss\n\n    def forward(self, pred, *targ):\n        losses = []\n        batch_size = pred[0].shape[0]\n        for pb, pc, tb, tc in zip(*pred, *targ):\n            tb, tc = self.unpad(tb, tc)\n            assignments = self.assign_anchors(self.anchors, tb)\n            box_mask = assignments &gt;= 0\n            if box_mask.sum() &gt; 0:\n                tb = tb[assignments[box_mask]]\n                an = self.anchors[box_mask]\n                pb = pb[box_mask]\n                pred_boxes = self.compute_pred_boxes(pb, an)\n                pred_boxes = box_convert(pred_boxes, 'cxcywh', 'xyxy')\n                box_loss = self.box_loss_func(pred_boxes, tb, reduction='mean')\n            else:\n                box_loss = 0.0\n            class_mask = assignments &gt;= -1\n            if class_mask.sum() &gt; 0:\n                class_assignments = tc[assignments[box_mask]] + 1\n                tc = self.compute_one_hot_targets(class_assignments, box_mask, class_mask)\n                class_loss = self.class_loss_func(pc[class_mask], tc.float(), reduction='sum')\n                class_loss = class_loss / box_mask.sum().clamp_min(1)\n            else:\n                class_loss = 0.0\n            losses.append(box_loss + class_loss)\n        return sum(losses) / batch_size\n\n    def unpad(self, targ_boxes, targ_classes):\n        mask = (targ_classes != -100)\n        return targ_boxes[mask], targ_classes[mask]\n\n    def assign_anchors(self, anchors, targ_boxes, foreground_thresh=0.5, background_thresh=0.4):\n        anchors = box_convert(anchors, in_fmt='cxcywh', out_fmt='xyxy')\n        iou_matrix = box_iou(anchors, targ_boxes)\n        max_iou, prelim_assignments = iou_matrix.max(dim=1)\n        foreground_mask = (max_iou &gt; foreground_thresh)\n        background_mask = (max_iou &lt; background_thresh)\n        assignments = torch.full((anchors.shape[0],), fill_value=-2, device=def_device)\n        assignments[foreground_mask] = prelim_assignments[foreground_mask]\n        assignments[background_mask] = -1\n        return assignments\n    \n    def compute_pred_boxes(self, box_preds, anchors):\n        result = torch.empty_like(anchors)\n        result[..., :2] = box_preds[..., :2] * anchors[..., 2:] + anchors[..., :2]\n        result[..., 2:] = anchors[..., 2:] * torch.exp(box_preds[..., 2:])\n        return result\n    \n    def compute_one_hot_targets(self, class_assignments, box_mask, class_mask):\n        result = torch.zeros(\n            (box_mask.shape[0],), device=class_assignments.device\n        )\n        result[box_mask] = class_assignments\n        result = result[class_mask]\n        return F.one_hot(result.long(), num_classes=21)[:, 1:]\n\n\n\nThe code for the loss function is partially based on code from this Keras tutorial and this notebook from an old version of the fastai course.\nFinally, we’ll create a fastai DataLoaders and train our model.\n\nobject_detection_dls = DataLoaders(\n    object_detection_train_dl, object_detection_valid_dl\n)\n\n# the Learner needs to know the number of inputs to the model\nobject_detection_dls.n_inp = 1\n\nobject_detection_backbone = multilabel_learn.model.backbone\n\nobject_detection_model = ObjectDetectionModel(\n    n_anchors=9, backbone=object_detection_backbone\n)\n\nobject_detection_learn = Learner(\n    dls=object_detection_dls,\n    model=object_detection_model,\n    loss_func=ObjectDetectionLoss(),\n).to_fp16()\n\nobject_detection_lrs = object_detection_learn.lr_find(\n    suggest_funcs=(minimum, steep, valley, slide)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nobject_detection_learn.fit_one_cycle(10, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.721214\n0.723528\n09:23\n\n\n1\n0.545930\n0.582645\n09:23\n\n\n2\n0.459409\n0.476774\n09:24\n\n\n3\n0.399870\n0.447026\n09:24\n\n\n4\n0.350284\n0.405484\n09:26\n\n\n5\n0.309018\n0.382009\n09:28\n\n\n6\n0.283841\n0.362980\n09:29\n\n\n7\n0.250841\n0.359760\n09:28\n\n\n8\n0.232529\n0.359746\n09:27\n\n\n9\n0.207503\n0.358098\n09:27\n\n\n\n\n\nNow let’s visualize the outputs of our trained RetinaNet.\n\n\nInference Image Plotting Functions\ndef draw_inference_bbox(ax, bbox, label, score, alpha=1.0):\n    x0, y0, x1, y1 = bbox\n    box = matplotlib.patches.Rectangle(\n        (x0, y0), x1 - x0, y1 - y0,\n        facecolor='none',\n        linewidth=2,\n        edgecolor=cmap(label),\n        alpha=alpha,\n        zorder=100*score.item(),\n    )\n    border_effects = [\n        path_effects.Stroke(linewidth=5, foreground='black', alpha=alpha),\n        path_effects.Normal()\n    ]\n    box.set_path_effects(border_effects)\n    ax.add_patch(box)\n\n\ndef add_bbox_inference_label(ax, bbox, label, score, alpha=1.0):\n    props = dict(\n        boxstyle='square',\n        facecolor=cmap(label),\n        edgecolor='black',\n        linewidth=1.1,\n        alpha=alpha,\n        zorder=100*score.item()\n    )\n    x0, y0, _, _ = bbox\n    if isinstance(label, torch.Tensor):\n        label = label.item()\n    ax.text(\n        x0, y0, idx2label[label] + f' {score.item():.2f}',\n        bbox=props,\n        color='black',\n        fontsize=8,\n        in_layout=True,\n        zorder=100*score.item()\n    )\n\n\ndef plot_image_with_inference_annotations(\n        image,\n        bboxes,\n        labels,\n        scores,\n        ax=None,\n        alpha=1.0\n    ):\n    image = decode(image)\n    if isinstance(bboxes, torch.Tensor):\n        bboxes = bboxes.cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.cpu().numpy()\n    if ax is None:\n        fig, ax = plt.subplots()\n        fig.tight_layout()\n    ax.imshow(image)\n    ax.axis('off')\n\n    for bbox, label, score in zip(bboxes, labels, scores):\n        draw_inference_bbox(ax, bbox, label, score, alpha=alpha)\n        add_bbox_inference_label(ax, bbox, label, score, alpha=alpha)\n\n\nfrom torchvision.ops import clip_boxes_to_image, batched_nms\n\n\ndef compute_pred_boxes(boxes, anchors):\n    result = torch.empty_like(anchors)\n    result[..., :2] = boxes[..., :2] * anchors[..., 2:] + anchors[..., :2]\n    result[..., 2:] = anchors[..., 2:] * torch.exp(boxes[..., 2:])\n    return result\n\n\ndef postprocess(box_preds, class_preds, detect_thresh, iou_thresh):\n        pred_scores, pred_classes = class_preds.max(dim=-1)\n        pred_scores = pred_scores.sigmoid()\n\n        idxs_to_keep = pred_scores &gt; detect_thresh\n        pred_boxes = box_preds[idxs_to_keep]\n        pred_classes = pred_classes[idxs_to_keep]\n        pred_scores = pred_scores[idxs_to_keep]\n\n        pred_boxes = clip_boxes_to_image(pred_boxes, (image_size, image_size))\n        \n        idxs_to_keep = batched_nms(\n            pred_boxes, pred_scores, pred_classes, iou_threshold=iou_thresh\n        )\n        pred_boxes = pred_boxes[idxs_to_keep]\n        pred_classes = pred_classes[idxs_to_keep]\n        pred_scores = pred_scores[idxs_to_keep]\n\n        return pred_boxes, pred_classes, pred_scores\n\n\ndef inference(model, batch_num, detect_thresh=0.35, iou_thresh=0.5):\n    valid_iter = iter(object_detection_valid_dl)\n    for _ in range(batch_num + 1):\n        batch = next(valid_iter)\n\n    images, targ_boxes, targ_classes = batch\n    batch_size = images.shape[0]\n    image_size = images.shape[-1]\n\n    model.eval()\n    with torch.no_grad():\n        boxes, classes = model(images.to(def_device))\n    \n    anchors = torch.stack([\n        generate_anchor_boxes(image_size).to(torch.float32)\n        for _ in range(batch_size)\n    ])\n    pred_boxes = compute_pred_boxes(boxes, anchors)\n    pred_boxes = box_convert(pred_boxes, 'cxcywh', 'xyxy')\n\n    fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(16, 32))\n    fig.tight_layout()\n    plot_data = zip(images, pred_boxes, classes, targ_boxes, targ_classes, axs)\n    for image, pred_box, pred_class, targ_box, targ_class, ax_row in plot_data:\n        pred_box, pred_class, pred_scores = postprocess(\n             pred_box, pred_class, detect_thresh, iou_thresh\n        )\n\n        targ_alpha = 1.0\n        non_padding = targ_class &gt; -100\n        targ_box = targ_box[non_padding]\n        targ_class = targ_class[non_padding].long()\n        \n        ax_row[0].imshow(decode(image))\n        ax_row[0].axis('off')\n        ax_row[0].set_title('Ground Truth')\n        for box, label in zip(targ_box, targ_class):\n            draw_bbox(ax_row[0], box.cpu(), label.cpu(), alpha=targ_alpha)\n            add_bbox_label(ax_row[0], box.cpu(), label.cpu(), alpha=targ_alpha)\n\n        plot_image_with_inference_annotations(\n            image=image,\n            bboxes=pred_box,\n            labels=pred_class,\n            scores=pred_scores,\n            ax=ax_row[1],\n        )\n        ax_row[1].set_title('Predictions')\n\n\n\ninference(\n    model=object_detection_learn.model,\n    batch_num=10,\n    detect_thresh=0.5,\n    iou_thresh=0.2\n)\n\n\n\n\n\n\n\n\n\n\nCOCO Metrics\nWe can get some quantitative estimates on how well our model performs using pycocotools.\n\n\nCOCO metric helper functions\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom fastai.vision.all import chunked\nimport json\n\n\nwith open(path_2007/'test.json') as f:\n    test_ground_truth = json.load(f)\n\n\ndef record_predictions(model, detect_thresh=0.35, iou_thresh=0.5):\n    results = []\n    batch_size = 8\n    image_size = 600\n    anchors = torch.stack([\n        generate_anchor_boxes(image_size).to(torch.float32)\n        for _ in range(batch_size)\n    ])\n    image_ids = [o['id'] for o in test_ground_truth['images']]\n    image_id_batches = chunked(image_ids, chunk_sz=8)\n    model.eval()\n\n    for batch, image_id_batch in zip(object_detection_valid_dl, image_id_batches):\n        images, *_ = batch\n\n        with torch.no_grad():\n            boxes, classes = model(images.to(def_device))\n        \n        pred_boxes = compute_pred_boxes(boxes, anchors)\n        pred_boxes = box_convert(pred_boxes, 'cxcywh', 'xyxy')\n\n        for pred_box, pred_class, image_id, in zip(pred_boxes, classes, image_id_batch):\n            pred_box, pred_class, pred_scores = postprocess(\n                pred_box, pred_class, detect_thresh, iou_thresh\n            )\n            pred_box = box_convert(pred_box, in_fmt='xyxy', out_fmt='xywh')\n            for cat_id, box, score in zip(pred_class, pred_box, pred_scores):\n                results.append({\n                    'image_id': image_id,\n                    'category_id': cat_id.item() + 1,\n                    'bbox': [round(o, 2) for o in box.tolist()],\n                    'score': round(score.item(), 3),\n                })\n    return results\n\n\ndef get_coco_scores(\n        model,\n        ground_truth_path=path_2007/'test.json',\n        annotation_type='bbox',\n        detect_thresh=0.35,\n        iou_thresh=0.5\n    ):\n    test_results = record_predictions(\n        model,\n        detect_thresh=detect_thresh,\n        iou_thresh=iou_thresh\n    )\n    with open('results/results.json', 'w') as f:\n        json.dump(test_results, f)\n    coco_ground_truth = COCO(ground_truth_path)\n    coco_predictions = coco_ground_truth.loadRes('results/results.json')\n    coco_eval = COCOeval(coco_ground_truth, coco_predictions, annotation_type)\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n\n\n\nget_coco_scores(\n    model=object_detection_learn.model,\n    detect_thresh=0.5,\n    iou_thresh=0.2\n)\n\nloading annotations into memory...\nDone (t=0.34s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.03s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=3.28s).\nAccumulating evaluation results...\nDONE (t=0.53s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.440\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.640\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.485\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.254\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.560\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.388\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.488\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.043\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.300\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.618\n\n\n\n\nResults on Example Image\nFinally, we’ll do inference on the example image we saw at the beginning of this post and get the bounding boxes we saw there.\n\nfrom fastai.vision.all import load_image\n\nimg = load_image('example_image.png', mode='RGB')\nimg = np.array(img)\nimg = multilabel_valid_tfms(image=img)['image']\nimg = torch.from_numpy(img)\nimg = img.permute(2, 0, 1)[None]\nimg = img.to(def_device)\n\nwith torch.no_grad():\n    boxes, classes = object_detection_learn.model(img)\n\nboxes, classes = boxes[0], classes[0]\nanchors = generate_anchor_boxes(600)\npred_boxes = compute_pred_boxes(boxes, anchors)\npred_boxes = box_convert(pred_boxes, 'cxcywh', 'xyxy')\nboxes, classes, scores = postprocess(\n    pred_boxes, classes, detect_thresh=0.5, iou_thresh=0.2\n)\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\naxs[0].imshow(load_image('example_image.png'))\naxs[0].axis('off')\naxs[0].set_title('Input Image')\nplot_image_with_inference_annotations(\n    load_image('example_image.png'), boxes, classes, scores, ax=axs[1]\n)\naxs[1].set_title('Model Predictions')\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "",
    "text": "The goal of this post is to do some data cleaning on the Oxford-IIIT Pets dataset, an image classification dataset with photos of cats and dogs. We’ll get our results using image embeddings from the body of a pre-trained ResNet model, some basic machine learning and image processing techniques, and a full ResNet model fine-tuned on the dataset using fastai. We’ll use fastai’s version of the Pets dataset and take advantage of some convenience functions from the fastai and fastcore libraries.\n\nimport os\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport cv2\n\nimport torch\nimport torchvision\nimport torch.nn.functional as F\n\nimport fastcore.all as fc\n\nThe list of Python packages and specific versions used to make this post are in the table below.\n\n\n\n\n\nPackage\nVersion\n\n\n\n\npython\n3.9.16\n\n\nscipy\n1.9.2\n\n\nfastai\n2.7.13\n\n\npytorch\n2.1.2+cu121\n\n\nfastcore\n1.5.29\n\n\ntorchvision\n0.16.2+cu121\n\n\nopencv-python\n4.9.0"
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#image-similarity-search",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#image-similarity-search",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Image Similarity Search",
    "text": "Image Similarity Search\nBefore we go any further, let’s implement a basic image similarity search function over the dataset. If our embeddings can’t accomplish that task, then using them to look for duplicates will be a waste of time. We’ll measure how similar two images are using the cosine similarity distance between their embeddings.\n\ndef pairwise_cosine_similarities(embs, batch_size=8, device=device):\n    embs = embs.to(device)\n    num_embs = embs.shape[0]\n    distances = torch.empty((num_embs, num_embs), device=device)\n    for idx in range(0, num_embs, batch_size):\n        s = slice(idx, min(idx+batch_size, num_embs))\n        distances[s] = F.cosine_similarity(embs[None], embs[s, None], dim=-1)\n    # don't match dataset entries with themselves\n    distances.fill_diagonal_(-torch.inf)\n    return distances.to('cpu')\n\ndistances = pairwise_cosine_similarities(embeddings)\n\nNow let’s look at some examples and see if our embeddings are worth anything.\n\ndef show_k_closest(idx, dataset, distances, k=4):\n    item_metrics = distances[idx]\n    k_closest_idxs = item_metrics.argsort(descending=True)[:k]\n    k_closest_imgs = dataset[k_closest_idxs]\n    relevant_imgs = [dataset[idx]] + k_closest_imgs\n    show_image_list(\n        relevant_imgs,\n        max_per_col=k+1,\n        title_option='label_original',\n    )\n\nshow_k_closest(500, pets_dataset, distances)\nshow_k_closest(3303, pets_dataset, distances)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL supports numpy-style indexing\nLooks good!"
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#duplicate-and-near-duplicate-images",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#duplicate-and-near-duplicate-images",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Duplicate and Near-Duplicate Images",
    "text": "Duplicate and Near-Duplicate Images\nNow let’s go ahead and find groups of similar images to hunt for duplicates. We’ll threshold the distances we computed earlier to get a boolean matrix, which we can view as the adjacency matrix of a graph whose vertices are the entries of the dataset.2 The connected components of that graph with more than one vertex are our desired groups.\n\nfrom scipy.sparse.csgraph import connected_components\n\ndef group_similar_images(distances, threshold, dataset):\n    graph = (distances &gt; threshold).numpy()\n    num_components, labels = connected_components(graph)\n    components = fc.L(dataset[labels == i] for i in range(num_components))\n    groups = components.filter(lambda o: len(o) &gt; 1)\n    return groups\n\ngroups = group_similar_images(distances, 0.85, pets_dataset)\n\n\n\nL has a built-in filter method.\nHow many images did we find?\n\nprint(f'number of groups: {len(groups):3d}\\n'\n      f'number of images: {len(groups.concat())}')\n\nnumber of groups:  92\nnumber of images: 192\n\n\n\n\nL has a built-in concat method that concatenates all of its elements into a new L\nWe can check and see that the groups we found really are duplicates or near-duplicates. We’ll only show a selection of the groups we found; the remaining ones are similar.\n\nshow_image_list(groups[20, 52, 44, 16, 2].concat(), max_per_col=6)\n\n\n\n\n\n\n\n\nLet’s update our list of images to remove all but one member from each group.\n\nduplicate_images = set(fc.flatten(o[1:] for o in groups))\nimages_to_remove.update(duplicate_images)\n\n\n\nas its name suggests, fc.flatten flattens its input, returning a generator"
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#bright-and-dark-images",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#bright-and-dark-images",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Bright and Dark Images",
    "text": "Bright and Dark Images\nOne way to measure the brightness of an image is to convert it to HLS (hue, lightness, and saturation) format. OpenCV has a cvtColor function that does this transformation. They compute lightness in the following way. For each pixel in an RGB image, with values \\((r, g, b)\\), we set \\(V_\\text{max} = \\max(r, g, b)\\) and \\(V_\\text{min} = \\min(r, g, b)\\); then the lightness of that pixel is the average \\((V_\\text{max} + V_\\text{min}) / 2\\).3\n\ndef brightness(img):\n    img = np.asarray(load_image(img, mode='RGB'))\n    img_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    return img_hls[..., 1].mean()\n\nbright_vals = fc.parallel(brightness, pets_dataset, n_workers=4)\n\n\n\nfc.parallel is a convenience multiprocessing function that maps a function passed as the first input over the second input in parallel\nWe’ll filter for bright images using a lower bound on the brightness and only show a few of the brightest images.4\n\nbright_images = pets_dataset[bright_vals.map(fc.gt(231))]\nshow_image_list(bright_images, row_height=4)\n\n\n\n\n\n\n\n\n\n\nfc.gt is a fastcore function that, when passed one parameter, returns a curried version of operator.gt\nThese images are also all clearly identifiable, so we’ll keep them too.\nWe can also filter for dark images using an upper bound on the brightness. Again, we’ll only show a few of the darkest images.\n\ndark_images = pets_dataset[bright_vals.map(fc.lt(25))]\nshow_image_list(dark_images, row_height=4)\n\n\n\n\n\n\n\n\n\n\nfc.lt is the “less than” analogue of the function fc.gt above\nThese images are all clearly identifiable, so we won’t remove any of them."
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#outliers",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#outliers",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Outliers",
    "text": "Outliers\nWe’ll look for potential outliers by first projecting our embeddings to a 50-dimensional space with principal component analysis and then using a local outlier factor model.5 The value for contamination is set to a low value here to show only a few images.6\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.pipeline import make_pipeline\n\npca = PCA(n_components=50, svd_solver='full')\nclf = LocalOutlierFactor(\n    n_neighbors=4, metric='cosine', n_jobs=4, contamination=0.0005\n)\noutlier_preds = make_pipeline(pca, clf).fit_predict(embeddings.numpy())\noutlier_candidates = pets_dataset[outlier_preds == -1]\nshow_image_list(outlier_candidates, row_height=4)\n\n\n\n\n\n\n\n\nOne of these images looks like a problem: great_pyrenees_36 contains both a cat and a dog, so we should remove it from the dataset.\n\nimages_to_remove.add(outlier_candidates[0])"
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#blurry-images",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#blurry-images",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Blurry Images",
    "text": "Blurry Images\nWe’ll measure blurriness using the squared Euclidean norm of the image Laplacian. A sharp image tends to have many distinct edges and boundaries in it, and the image Laplacian detects those features.\n\ndef compute_laplacian(img):\n    img = np.asarray(load_image(img, mode='L'))\n    return cv2.Laplacian(img, cv2.CV_64F, ksize=5)\n\nThe way the Laplacian detects blurry images is most clearly seen through examples. We’ll start with a relatively sharp image…\n\nsharp_img = pets_dataset[59]\nsharp_laplacian = compute_laplacian(sharp_img)\nshow_image_list([sharp_img, sharp_laplacian], col_width=11, row_height=6)\n\n\n\n\n\n\n\n\n… and contrast that with a relatively blurry image.\n\nblurry_img = pets_dataset[5674]\nblurry_laplacian = compute_laplacian(blurry_img)\nshow_image_list([blurry_img, blurry_laplacian], col_width=6, row_height=4)\n\n\n\n\n\n\n\n\nSharp images tend to have a Laplacian with a greater squared norm than blurry images. We’ll just compute it for each image and take those with the lowest values as our candidate blurry images. I’ll only show a few of the images whose Laplaicans have the smallest squared norms.7\n\ndef laplacian_norm_squared(img):\n    laplacian = compute_laplacian(img)\n    return (laplacian**2).sum()\n\nsquared_norms = fc.parallel(laplacian_norm_squared, pets_dataset, n_workers=4)\nthreshold = np.quantile(squared_norms, q=0.0005)\nblurry_candidates = pets_dataset[squared_norms.map(fc.lt(threshold))]\nshow_image_list(blurry_candidates, max_per_col=4, row_height=4)\n\n\n\n\n\n\n\n\n\n\nL has a built-in map method\nEach of these images is identifiable, so we’ll keep them all."
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#fine-tune-a-model",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#fine-tune-a-model",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Fine-Tune a Model",
    "text": "Fine-Tune a Model\nFinally, we’ll use fastai to quickly fine-tune a breed classification model on the Pets dataset and examine the images with the greatest losses to check for any other issues. We’ll use fastai’s DataBlock API to organize our data into a DataLoaders and the vision_learner convenience function to create a trainer that we’ll use to fine-tune a pre-trained ResNet50 model.8 One of fastai’s best features is its learning rate finder, which, as the name suggests, helps us find an advantageous learning rate.\n\nimport fastai.vision.all as fv\n\ndef get_breed(filename):\n    return '_'.join(filename.stem.split('_')[:-1])\n\ndblock = fv.DataBlock(\n    blocks=(fv.ImageBlock, fv.CategoryBlock),\n    get_items=fc.noop,\n    get_y=get_breed,\n    item_tfms=fv.Resize(460),\n    batch_tfms=[*fv.aug_transforms(size=224, min_scale=0.75),\n                fv.Normalize.from_stats(*fv.imagenet_stats)],\n)\ndls = dblock.dataloaders(pets_dataset)\nlearn = fv.vision_learner(dls, fv.resnet50, metrics=fv.accuracy)\nlrs = learn.lr_find(suggest_funcs=(fv.minimum, fv.steep, fv.valley, fv.slide))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt looks like the loss is decreasing most quickly at the point marked steep, so we’ll use that learning rate. We’ll use the Learner’s fine_tune method to first freeze the weights in the model’s body and train for one epoch, then unfreeze everything and train for five epochs.\n\nlearn.fine_tune(5, lrs.steep)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.126889\n0.367208\n0.889716\n00:17\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.419063\n0.284448\n0.921516\n00:20\n\n\n1\n0.365594\n0.341063\n0.899188\n00:20\n\n\n2\n0.255289\n0.224743\n0.929635\n00:20\n\n\n3\n0.140554\n0.210314\n0.932341\n00:20\n\n\n4\n0.081783\n0.204320\n0.939784\n00:20\n\n\n\n\n\nWe can use fastai’s Interpretation class to look at the images where the outputs from the model have the greatest losses.\n\nplt.rcParams.update({'font.size': 8})\ninterp = fv.Interpretation.from_learner(learn)\ninterp.plot_top_losses(k=9, nrows=3, figsize=(15, 9))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere don’t seem to be any issues with the images themselves, so we won’t remove anything from the dataset."
  },
  {
    "objectID": "blog/posts/2024-01-08-clean-pets-dataset/index.html#footnotes",
    "href": "blog/posts/2024-01-08-clean-pets-dataset/index.html#footnotes",
    "title": "Cleaning an Image Dataset with ResNet Embeddings and fastai",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI tried out a few reasonably sized pre-trained models from torchvision, fastai, and timm, and the embeddings from this Torchvision ResNet50 model seemed to perform the best on image similarity search and finding duplicate images.↩︎\nI chose the threshold of 0.85 to try to catch as many duplicate groups as I could while making sure that near-duplicate groups had only small changes between the images.↩︎\nI found that OpenCV’s implementation was faster than using the formula directly.↩︎\nI didn’t find any problem images with a lower threshold either.↩︎\nOf the outlier detection methods I tried, this was the one that most easily found the problem image below.↩︎\nI didn’t find any additional problem images with higher values for contamination.↩︎\nI didn’t find any problem images with higher threshold values.↩︎\nWe’re using fastai’s ResNet50 pre-trained weights now since they fine-tuned more easily and to a higher accuracy than the Torchvision weights we used earlier.↩︎"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "A Dashboard for Crime in Los Angeles\n\n\n\n\n\n\nDashboard\n\n\npandas\n\n\nGeoPandas\n\n\nPlotly\n\n\nShiny\n\n\n\nA Shiny dashboard for crimes reported and arrests made in Los Angeles from 2020 through 2024.\n\n\n\n\n\nAug 31, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing DiffEdit: Editing Images with Text Prompts\n\n\n\n\n\n\nComputer Vision\n\n\nDiffusion Models\n\n\nGenerative AI\n\n\n\nWe’ll implement DiffEdit by hand and show what it can do.\n\n\n\n\n\nApr 11, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding and Training a RetinaNet\n\n\n\n\n\n\nComputer Vision\n\n\nObject Detection\n\n\n\nWe’ll build a RetinaNet model and train it on PASCAL VOC.\n\n\n\n\n\nMar 25, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Hugging Face Image Datasets with fastai\n\n\n\n\n\n\nComputer Vision\n\n\nHugging Face\n\n\n\nWe’ll look at different ways of using fastai’s data pipeline with Hugging Face image datasets and find an efficient option.\n\n\n\n\n\nFeb 12, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning an Image Dataset with ResNet Embeddings and fastai\n\n\n\n\n\n\nComputer Vision\n\n\n\nWe’ll clean up some problems in the Oxford-IIIT Pets dataset.\n\n\n\n\n\nJan 8, 2024\n\n\nJacob Rooney\n\n\n\n\n\n\nNo matching items"
  }
]